
#from .version import version as __version__

# __all__ = []  # defines which functions, variables etc. will be loaded when running "from pyonset import *"

"""
A library that holds the Onset class for PyOnset.

@Author: Christian Palmroos <chospa@utu.fi>

@Updated: 2025-10-01

Known problems/bugs:
    > Does not work with SolO/STEP due to electron and proton channels not defined in all_channels() -method
"""

# For checking Python version
import sys

import os
import warnings
import datetime

import astropy.units as u
import matplotlib.ticker as ticker
import numpy as np
import pandas as pd
from scipy.odr import ODR, Model, RealData
from pandas.tseries.frequencies import to_offset
from astropy import constants as const
from matplotlib import pyplot as plt
from matplotlib import rcParams
from matplotlib.dates import DateFormatter, DayLocator, HourLocator
from matplotlib.offsetbox import AnchoredText
from sunpy.util.net import download_file

from sunpy import __version__

warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=UserWarning)

from IPython.display import Markdown

import seppy.util as util
from seppy.loader.psp import (calc_av_en_flux_PSP_EPIHI,
                              calc_av_en_flux_PSP_EPILO, psp_isois_load)
from seppy.loader.soho import calc_av_en_flux_ERNE, soho_load
from seppy.loader.stereo import calc_av_en_flux_HET as calc_av_en_flux_ST_HET
from seppy.loader.stereo import calc_av_en_flux_SEPT, stereo_load
from seppy.tools import Event

# Local import
from .bootstrapwindow import BootstrapWindow, subwindows
from .onsetstatsarray import OnsetStatsArray

from .datetime_utilities import datetime_to_sec, datetime_nanmedian, detrend_onsets, \
                                get_time_reso, calculate_cusum_window, find_biggest_nonzero_unit, \
                                get_figdate, check_confidence_intervals

from .calc_utilities import z_score, sigma_norm, k_parameter, k_legacy, k_classic
from .plot_utilities import set_fig_ylimits, set_standard_ticks, set_legend, max_averaging_reso_textbox, save_figure, \
                            midnight_format_ticks, TITLE_FONTSIZE, STANDARD_FIGSIZE, VDA_FIGSIZE, AXLABEL_FONTSIZE, \
                            TICK_LABELSIZE, TXTBOX_SIZE, LEGEND_SIZE, COLOR_SCHEME

__author__ = "Christian Palmroos"
__email__ = "chospa@utu.fi"

# Some useful global constants
CURRENT_PATH = os.getcwd()
C_SQUARED = const.c.value*const.c.value

ELECTRON_IDENTIFIERS = ("electrons", "electron", 'e')
PROTON_IDENTIFIERS = ("protons", "proton", "ions", "ion", 'p', 'i', 'H')

SEPPY_SPACECRAFT = ("sta", "stb", "solo", "psp", "wind", "soho", "bepi")
SEPPY_SENSORS = {"sta" : ("sept", "het"),
                 "stb" : ("sept", "het"),
                 "solo" : ("ept", "het"),
                 "psp" : ("isois_epilo", "isois_epihi"),
                 "wind" : ("3dp"),
                 "soho" : ("erne-hed", "ephin"),
                 "bepi" : ("sixs-p")
                 }

# SOHO / EPHIN e300 channel is invalid from this date onwards
EPHIN_300_INVALID_ONWARDS = pd.to_datetime("2017-10-04 00:00:00")

CUSUM_WINDOW_RESOLUTION_MULTIPLIERS = (4,8,16,32)

# A fine cadence is less than 1 minute -> requires computation time
FINE_CADENCE_SC = ("solo", "wind")

# Save newline character to a constant to use 
# in f-strings with older python versions
NEWLINE = "\n"

# A new class to inherit everything from serpentine Event, and to extend its scope of functionality
class Onset(Event):

    def __init__(self, start_date, end_date, spacecraft, sensor, species, data_level, data_path, viewing=None, radio_spacecraft=None, threshold=None,
                 data=None, unit=None):

        # By default we download data, not provide it
        if data is None:
            super().__init__(start_date, end_date, spacecraft, sensor,
                    species, data_level, data_path, viewing, radio_spacecraft,
                    threshold)
            self.custom_data = False
            self.unit = r"Intensity [1/(cm$^{2}$ sr s MeV)]" if unit is None else unit

            # Check here that the spacecraft and instrument are SEPpy-compatible.
            # to provide the custom data.
            if self.spacecraft not in SEPPY_SPACECRAFT:
                raise Exception(f"Note that {self.spacecraft} is not a SEPpy-compatible spacecraft. You need to provide your own data with the keyword: 'data'.")

        else:

            # Copy-pasted from seppy.tools.Event initializer, to keep internal format of variables
            # consistent
            if spacecraft == "Solar Orbiter":
                spacecraft = "solo"
            if spacecraft == "STEREO-A":
                spacecraft = "sta"
            if spacecraft == "STEREO-B":
                spacecraft = "stb"
            if spacecraft.lower()=="bepicolombo":
                spacecraft = "bepi"

            if sensor in ["ERNE-HED"]:
                sensor = "ERNE"

            if species in PROTON_IDENTIFIERS:
                species = 'p'
            if species in ELECTRON_IDENTIFIERS:
                species = 'e'

            self.start_date = start_date
            self.end_date = end_date
            self.spacecraft = spacecraft
            self.sensor = sensor
            self.species = species
            self.data_level = data_level
            self.data_path = data_path
            self.viewing = viewing
            self.radio_spacecraft = radio_spacecraft
            self.threshold = threshold

            if not isinstance(data,pd.DataFrame):
                raise TypeError("Custom data needs to be provided as a DataFrame indexed by time!")
            self.data = data.copy(deep=True)
            self.current_df_e = self.data
            self.current_df_i = self.current_df_e
            self.unit = r"Intensity [1/(cm$^{2}$ sr s MeV)]" if unit is None else unit

            # Custom data flag prevents SEPpy functions from being called, as they would cause errors
            self.custom_data = True

            # The channel energy dictionary maps channel names to channel energies
            self.channel_en_dict = None

            # Lets the user know that the object is initialized with custom settings
            print("Utilizing user-input data. Some SEPpy functionality may not work as intended.")

        # Everytime an onset is found any way, the last used channel should be updated
        self.last_used_channel = np.nan

        # The background window is stored to this attribute when cusum_onset() is called with a BootStrapWindow input
        self.background = None

        # This list is for holding multiple background windows if such were to be used
        self.list_of_bootstrap_windows = []
        self.window_colors = ["blue", "orange", "green", "purple", "navy", "maroon"]

        # This will turn true once the extensive statistics analysis is run
        self.mean_of_medians_onset_acquired = False

        # Let the object remember which n was used to calculate the k in the method.
        # By default this value is 2, but update it if changed.
        self.sigma_multiplier = 2

        # Choosing the particle species identifier for the titles etc
        if self.species in ["electron", "electrons", 'e']:
            self.species = 'e'
            self.s_identifier = "electrons"
        elif self.species in ["proton", "protons", 'p', 'H']:
            self.species = 'p'
            self.s_identifier = "protons"
        elif self.species in ["ion", "ions", 'i']:
            self.species = 'p'
            self.s_identifier = "ions"
        else:
            self.s_identifier = self.species

        # This is a check to make sure viewing is correctly set for single-aperture instruments
        if self.viewing:
            self.check_viewing()

        self.all_channels = {
            "sta_let_e" : np.arange(1, dtype=int),
            "sta_sept_e" : np.arange(2,17, dtype=int),
            "sta_het_e" : np.arange(3, dtype=int),
            "sta_let_p" : np.arange(1, dtype=int),
            "sta_sept_p" : np.arange(2,32, dtype=int),
            "sta_het_p" : np.arange(11, dtype=int),
            
            "stb_let_e" : np.arange(1, dtype=int),
            "stb_sept_e" : np.arange(2,17, dtype=int),
            "stb_het_e" : np.arange(3, dtype=int),
            "stb_let_p" : np.arange(1, dtype=int),
            "stb_sept_p" : np.arange(2,32, dtype=int),
            "stb_het_p" : np.arange(11, dtype=int),

            "solo_step_e" : np.arange(1, dtype=int),
            "solo_ept_e" : np.arange(34, dtype=int),
            "solo_het_e" : np.arange(4, dtype=int),
            "solo_step_p" : np.arange(1, dtype=int),
            "solo_ept_p" : np.arange(64, dtype=int),
            "solo_het_p" : np.arange(36, dtype=int),

            "soho_ephin_e" : (150, 300, 1300, 3000),
            "soho_erne_p" : np.arange(10, dtype=int),

            "wind_3dp_e" : np.arange(7, dtype=int),
            "wind_3dp_p" : np.arange(9, dtype=int),

            "psp_isois-epihi_e" : np.arange(19, dtype=int),
            "psp_isois-epihi_p" : np.arange(15, dtype=int),

            "psp_isois-epilo_e" : np.arange(12, dtype=int),
        }

        # This dictionary holds the median onset time and weighted confidence intervals derived by CUSUM-bootstrap 
        # hybrid method. In the dictionary each channel identifier is paired with a list of [mode, median, 1st_minus, 1st_plus, 2nd_minus, 2nd_plus]
        self.onset_statistics = {}

        # A dictionary that holds the maximum averaging times for each channel
        self.max_avg_times = {}

        # This is a dictionary that holds the information of each instrument's minimum time resolution.
        # In the current state of the software this dictionary does not serve any real practical purpose.
        self.minimum_cadences = {
            "bepicolombo_sixs-p" : pd.Timedelta("8 s"),
            "psp_isois-epihi" : pd.Timedelta("1 min"),
            "psp_isois-epilo" : pd.Timedelta("1 min"),
            "soho_erne" : pd.Timedelta("1 min"),
            "soho_ephin" : pd.Timedelta("1 min"),
            "sta_sept" : pd.Timedelta("1 min"),
            "sta_het" : pd.Timedelta("1 min"),
            "stb_sept" : pd.Timedelta("1 min"),
            "stb_het" : pd.Timedelta("1 min"),
            "solo_step" : pd.Timedelta("1 s"),
            "solo_ept" : pd.Timedelta("1 s"),
            "solo_het" : pd.Timedelta("1 s"),
            "wind_3dp" : pd.Timedelta("12 s")
        }

    def check_viewing(self, returns=False):
        """
        A method that checks at initialization that a viewing direction wasn't erroneously appointed
        for an instrument that doesn't have viewing directions.
        Sets self.viewing to None for STA/STB HET and all SOHO instruments (ERNE and EPHIN)

        returns : {bool}
                    Switch to return the viewing
        """

        if self.sensor.lower() in ("het", "erne", "ephin"):

            # STA/STB HET, and SOHO all instruments don't have viewings -> assign None to them
            if self.spacecraft.lower() in ("sta", "stb", "soho"):
                self.viewing = None
            else:
                self.viewing = self.viewing

        # Sixs has sides, but the input is the index of the side. Check that it's a char, not an int.
        if self.sensor.lower()=="sixs-p":
            if isinstance(self.viewing,int):
                self.viewing = str(self.viewing)

        if returns:
            return self.viewing

    def __repr__(self):
        return str(f"({self.spacecraft},{self.sensor},{self.species})")

    def get_all_channels(self):
        """
        Returns a range(first,last+1) of all the channel identifiers for any unique instrument+species pair.

        KeyError is thrown if the spacecraft+sensor+species -combination doesn't correspond to any of the
        recognized channel lists. This is handled by using the self.data (pd.DataFrame) column names.
        """

        try:
            return self.all_channels[f"{self.spacecraft}_{self.sensor}_{self.species}"]
        except KeyError:
            return np.array(self.data.columns)

    def get_current_channel_str(self):
        """
        Gets the string for the current energy channel
        """
        return self.get_channel_energy_values(returns="str")[self.last_used_channel]

    def get_minimum_cadence(self):
        try:
            return self.minimum_cadences[f"{self.spacecraft}_{self.sensor}"]
        except KeyError:
            # return pd.Timedelta(self.data.index.freq)
            return pd.Timedelta(get_time_reso(self.data))

    # Comment out, to be deleted in a later patch for redundancy.
    # def get_time_resolution_str(self, resample):
    #     # Choose resample as the averaging string if it exists
    #     if resample:
    #         time_reso_str = f"{resample} averaging" 
    #     # If not, check if the native cadence is less than 60. If yes, address the cadence in seconds
    #     elif self.get_minimum_cadence().seconds<60:
    #         time_reso_str = f"{self.get_minimum_cadence().seconds} s data"
    #     # If at least 60 seconds, address the cadence in minutes
    #     else:
    #         try:
    #             time_reso_str = f"{int(self.get_minimum_cadence().seconds/60)} min data"
    #         except ValueError:
    #             time_reso_str = "Unidentified time resolution"

    #     return time_reso_str

    def get_custom_channel_energies(self):
        """
        Gets the energy values that have been set by the user.
        Returns energy channel low and high boundaries in eVs.

        Handles the AttributeError that is caused by non-defined energy boundary
        values by printing the error, an explanation of how to fix it and returning None.
        """

        try:
            return self.channel_energy_lows, self.channel_energy_highs
        except AttributeError as e:
            print(e)
            print("This is caused by user-defined custom data, that has no defined energy channel boundary values.")
            print("Define energy channel boundaries with 'set_custom_channel_energies()'!")
            return None

    def _get_species_identifier(self):
        """
        Gets (returns) the plural form, particle species string identifier, e.g., 'electrons'.
        """
        if self.species in ELECTRON_IDENTIFIERS:
            return "electrons"
        if self.species in ["proton", 'p', 'H']:
            return "protons"
        if self.species in ["ion", 'i']:
            return "ions"

    def _get_title(self, energy_str:str, time_resolution:str=None):
        """
        Generates the title string for an onset plot.

        Parameters:
        -----------
        energy_str : {str}
                    The energy string of the corresponding channel.
        time_resolution : {str}, optional
                    The time resolution of the data. Will be added in parentheses
                    at the end of the title NOT on its own line.
        """

        sc_abbreviations_dict = {
            "sta" : "STEREO-A",
            "stb" : "STEREO-B",
            "solo" : "Solar Orbiter",
            "soho" : "SOHO",
            "wind" : "Wind",
            "psp" : "Parker Solar Probe",
            "bepi" : "BepiColombo"
        }

        species_abbreviations_dict = {
            'e' : "Electrons",
            'p' : "Protons",
            'H' : "Protons",
            'i' : "Ions"
        }

        spacecraft = self.spacecraft
        sensor = self.sensor
        viewing = self.viewing
        species = self.species

        # Correct spacecraft name. Always get the correct name if spacecraft is in SEPPY,
        if spacecraft in SEPPY_SPACECRAFT:
            spacecraft = sc_abbreviations_dict[spacecraft]

        # Correct species name
        if species in species_abbreviations_dict.keys():
            species = species_abbreviations_dict[species]

        # Make sensor all-caps
        sensor = sensor.upper()
        
        # Check that viewing is in correct format. Empty str for None
        viewing = viewing if viewing is not None else ""

        # Wind viewing format:
        if spacecraft=="Wind":
            if len(viewing)==1:
                viewing = f"Sector {viewing}"

        # Bepi viewing format:
        if spacecraft=="BepiColombo":
            viewing = f"Side{viewing}"

        title_str = f"{spacecraft} / {sensor}$^{{\\mathrm{{{viewing}}}}}${NEWLINE}{energy_str} {species}"

        # Add the time resolution at the end of the title, if asked to
        if isinstance(time_resolution, str):
            title_str += f" ({time_resolution} data)"

        # Return the complete title str
        return title_str

    def add_bootstrap_window(self, window):
        """
        A little helper method to add bootstrapping windows to the class list
        """
        self.list_of_bootstrap_windows.append(window)

    def update_onset_statistics(self, stats):
        """
        Adds the weighted median and confidence intervals of a specific channel to onset_statistics dictionary
        """

        self.onset_statistics[self.last_used_channel] = stats

    def input_nat_onset_stats(self, key):
        """
        This method can be used to e.g., fill in SolO/EPT channel 0 stats, which is unavailable in latest data.
        It may also be used to erase clearly erroneous onset time data from the object.
        """
        nan_stats = [pd.NaT, pd.NaT, pd.NaT, pd.NaT, pd.NaT, pd.NaT]
        self.onset_statistics[key] = nan_stats

    def set_data_frequency(self, freq):
        """
        Tries to set a frequency for the data. Useful especially incase of custom data.

        Parameters:
        -----------
        freq : {str}
                A pandas-compatible time string that represents the frequency of 
                the time series data, e.g., '24 s' or '1 min'.
        """
        self.data.index.freq = freq

    def set_channel_strings(self, channel_lows, channel_highs, unit):
        """
        Sets the dictionary of channel energy strings, e.g., '45 - 55 keV'.
        """
        channel_en_dict = {}
        for i, low in enumerate(channel_lows):
            
            channel_en_dict[self.data.columns[i]] = f"{low} - {channel_highs[i]} {unit}"
        
        self.channel_en_dict = channel_en_dict

    def set_custom_channel_energies(self, low_bounds, high_bounds, unit="MeV"):
        """
        Sets the channel energy boundary values. Automatically updates self.channel_en_dict
        
        Parameters:
        -----------
        low_bounds : {array-like}
        
        high_bounds : {array_like}
        
        unit : {str} default 'MeV'. Choose either 'eV', 'keV' or 'MeV'.
        """

        if len(low_bounds) != len(self.data.columns) or len(high_bounds) != len(self.data.columns):
            raise ValueError("Either low_bounds or high_bounds has an incorrect amount of entries!")

        if unit not in ("eV", "keV", "MeV"):
            raise ValueError(f"Unit {unit} does not appear to be any of the recognized units of energy! Choose either 'eV', 'keV' or 'MeV'.")

        if unit == "MeV":
            self.channel_energy_lows = np.array([low*u.MeV.to("eV") for low in low_bounds])
            self.channel_energy_highs = np.array([high*u.MeV.to("eV") for high in high_bounds])

        if unit == "keV":
            self.channel_energy_lows = np.array([low*u.keV.to("eV") for low in low_bounds])
            self.channel_energy_highs = np.array([high*u.keV.to("eV") for high in high_bounds])

        if unit == "eV":
            self.channel_energy_lows = np.array([low for low in low_bounds])
            self.channel_energy_highs = np.array([high for high in high_bounds])

        self.set_channel_strings(channel_lows=low_bounds, channel_highs=high_bounds, unit=unit)


    def cusum_onset(self, channels, background_range, viewing=None, resample=None, cusum_minutes=30, 
                    sigma_multiplier=2, title=None, save=False, savepath=None, yscale='log', 
                    ylim=None, erase=None, xlim=None, show_stats=True, diagnostics=False, 
                    plot=True, fname:str=None, k_model:str=None, poisson_test:bool=False, 
                    norm='z', cusum_type:str=None):
        """
        Does a Poisson-CUSUM-method-based onset analysis for given OnsetAnalysis object.
        Based on an earlier version by: Eleanna Asvestari <eleanna.asvestari@helsinki.fi>

        Parameters
        ----------
        channel: int or a list of 2 ints
                Index of the channel inside the DataFrame that holds the data.
        background_range : tuple/list or {BootstrapWindow}
                The starting and ending points of what is considered background. 
        resample: str
                Pandas-compatible time string, e.g., '1min' or '30s'
        cusum_minutes: int
                The amount of minutes the intensity should stay above threshold until onset is identified. Corresponds to the amount
                of consecutive "out-of-control" alarms one should get before finding the onset.
        sigma_multiplier: int, default 2
                How many standard deviations is the mu_d variable of the CUSUM function?
        title: str, default None
                A custom title for your plot.
        save: bool
                Wether or not to save the figure in local "plots" directory. Only works if plot=True.
        savepath: str, default None
                The path to which to save the plot, if not the default "plots".
        yscale: str
                The scale of the y-axis, either 'log' or 'linear'.
        ylim: 2-tuple, default None
                Defines the lower and upper bounds for the plots vertical axis.
        erase: 2-tuple, default None
                If there are spikes in the background one wishes to omit, set the threshold value and ending point for ignoring those
                values here. Example: [1000, '2022-05-10 12:00'] discards all values 1000 or above until 2022-05-10 noon.
        xlim: 2-tuple (str, str), default None
                Pandas-compatible dtateime strings. Defines the start and end for the horizontal axis of the plot.
        show_stats : bool, default True
                Switch to show the mean of medians onset and confidence intervals. Also shows the earliest possible onset (first nonzero CUSUM value)
        diagnostics: bool, default False
                Diagnostic tool allows for plotting the CUSUM function, and printing many 'behind the curtains' variables for the user to see.
        plot: bool, default True
                Switch to produce a plot. 
        fname : {str} default None
                A custom name for the figure if saved.
        k_model : {Callable, str} default None.
                Choose a custom model for the k-parameter. Input must either be a function that calculates k with
                identical signature to the 'k_parameter()' found in this software, or the word 'legacy' to chooce
                the old SEPpy k-parameter.
        poisson_test : {bool}, default False
        norm : {str} Either 'z' for z-standardization or 'sigma' for stadardization to std.
        cusum_type : {str} Choose either 'classic' for the classical definition of CUSUM (designed for integer numbers), or 'modified' for the
                modified CUSUM that works on z-standardized values. Default == None -> modified.
        Returns:
        ---------
        onset_stats: {dict}
                A dictionary containing onset statistics. {background_mean, background_mean+sigma*std, k-parameter,
                h-parameter, normed_values, CUSUM_function, Timestamp_of_Onset, channel_energy_string}
        flux_series: Pandas Series
                The series used for producing the plot and running the onset analysis.
        """

        spacecraft = self.spacecraft.upper()

        # Set the viewing even in the case it was not input by a user
        viewing = viewing if viewing else self.viewing

        # Logic of this check:
        # If viewing was set (meaning viewing evaluates to True) and Onset.check_viewing() returns None 
        # (which evaluates to False), then viewing must be invalid.
        if viewing and not self.check_viewing(returns=True):
            raise ValueError("Invalid viewing direction!")

        color_dict = {
            'onset_time': '#e41a1c',
            'bg_mean': '#e41a1c',
            'flux_peak': '#1a1682',
            'bg': '#de8585'
        }

        # This check was initially transforming the 'channels' integer to a tuple of len==1, but that
        # raised a ValueError with solo/ept. However, a list of len==1 is somehow okay.
        # Also save the channel to a class attribute for later use
        if isinstance(channels, (int, np.int64)):
            self.last_used_channel = int(channels)
            channels = [int(channels)]

        # Background as a list/tuple of datetime strings
        if isinstance(background_range,(list,tuple)):
            background_start, background_end = pd.to_datetime(background_range[0]), pd.to_datetime(background_range[1])

        # By default we do not use custom data
        if not self.custom_data:
            flux_series, en_channel_string = self.choose_flux_series(channels=channels, viewing=viewing)
        else:
            flux_series = self.data[channels]

            # By default we first try to access the channel energies from the channel energies
            # dictionary. If that fails, fall back to using the channel name for channel energy.
            try:
                en_channel_string = self.channel_en_dict[channels]
            
            # TypeError is caused by NoneType being unsubscriptable; the dictionary does not exist
            except TypeError:
                print("Channel energy dictionary not found.")
                print("Define channel energy boundaries with 'set_custom_channel_energies().")

            # KeyError is raised when there is not corresponding channel energy string for the
            # given channel.
            except KeyError as kee_ee:
                print(kee_ee)
                print("Check that channel energies are set correctly.")
            finally:
                en_channel_string = channels

            self.last_used_channel = channels

        # Save the native resolution to a class attribute.
        self.native_resolution = get_time_reso(series=flux_series)

        # Glitches from the data should really be erased BEFORE resampling data
        if erase is not None:
            flux_series, glitches = erase_glitches(flux_series, erase[0], erase[1])
        else:
            glitches = None

        # Resample data if requested
        if resample is not None:
            flux_series = util.resample_df(flux_series, resample)

        # Here just to make sure, check that the flux series is cut correctly
        if len(flux_series) == 0:
            raise Exception("Intensity data selection is empty! Check that plot limits are correct.")

        # Checking of background
        if isinstance(background_range, BootstrapWindow):
            background_start, background_end = background_range.start, background_range.end
            self.background = background_range

            # Attach the selected segment of flux_series to the BootstrapWindow.
            background_range.apply_background_selection(flux_series=flux_series)

            # The results of this test is returned but not really used anywhere
            if poisson_test:
                poisson_test_result = background_range.fast_poisson_test()

        # The series is indexed by time
        time = flux_series.index

        # Saves the intensity time series to a class attribute, handy if debugging or such is needed
        self.flux_series = flux_series

        # Onset is not yet found
        onset_found = False

        # Background stats are a tuple of (mean, standard_deviation)
        background_stats = calculate_mean_and_std(background_start, background_end, flux_series)

        # Also check that the background parameters are calculated using a reasonable background_window
        if background_stats[0] == 0 or np.isnan(background_stats[0]):
            background_warning = f"WARNING: background mean is {background_stats[0]}! If this was unintended, it may have been caused by an incorrect choice of background window."
            warnings.warn(message=background_warning)

        # Calculate the length of cusum_window in terms of data points from the time resolution
        time_reso = get_time_reso(flux_series)
        cusum_window = calculate_cusum_window(time_reso, cusum_minutes)

        # The function finds the onset and returns a list of stats related to the onset
        # onset_stats = [ma, md, k_round, h, norm_channel, cusum, onset_time]
        if cusum_type is None or cusum_type=="modified":
            onset_stats = onset_determination(background_stats, flux_series, cusum_window, background_end, sigma_multiplier, k_model=k_model)
        elif cusum_type=="classic":
            # If the unit is count rate (1/s), then employ Poisson-CUSUM without using z-standardized intensity
            onset_stats = onset_determination_cr(background_stats, flux_series, cusum_window, background_end, sigma_multiplier)
        else:
            raise ValueError(f"The parameter cusum_type must be either 'modified' or 'classic', not {cusum_type}. None=='modified'.")

        # Update the class attribute n after the onset has been found. This is done AFTER onset determimination
        # so that an invalid value of n may not be saved.
        self.sigma_multiplier = sigma_multiplier

        # If the timestamp of onset is not NaT, then onset was found
        if not isinstance(onset_stats[-1],pd._libs.tslibs.nattype.NaTType):
            onset_found = True

        # Prints out useful information if diagnostics is enabled
        if diagnostics:
            print(f"Cusum_window, {cusum_minutes} minutes = {cusum_window} datapoints")
            print(f"onset time: {onset_stats[-1]}")
            print(f"mu and sigma of background intensity: {NEWLINE}{background_stats[0]:.3e}, {background_stats[1]:.3e}")

        # --Only plotting related code from this line onward ->

        # Before starting the plot, save the original rcParam options and update to new ones
        original_rcparams = self.save_and_update_rcparams("onset_tool")

        if plot:
            fig, ax = plt.subplots(figsize=STANDARD_FIGSIZE)

            # Setting the x-axis limits
            if xlim is None:
                xlim = [time[0], time[-1]]
            else:

                # Check that xlim makes sense
                if xlim[0] == xlim[1]:
                    raise Exception("xlim[0] and xlim[1] cannot be the same time!")

                xlim[0], xlim[1] = pd.to_datetime(xlim[0]), pd.to_datetime(xlim[1])

            # Just make sure that the user did not mess up their x-axis limits and the background for the cusum function
            if (background_range is not None) and (xlim is not None):
                # Check if background is separated from plot range by over a day, issue a warning if so, but don't
                if (background_start < xlim[0] - datetime.timedelta(days=1) and background_start < xlim[1] - datetime.timedelta(days=1)) or \
                (background_end > xlim[0] + datetime.timedelta(days=1) and background_end > xlim[1] + datetime.timedelta(days=1)):
                    background_warning = f"NOTICE that your background_range is separated from plot_range by over a day.{NEWLINE}If this was intentional you may ignore this warning."
                    warnings.warn(message=background_warning)

            # Figure limits and scale
            flux_in_plot = flux_series.loc[(flux_series.index < xlim[-1])&(flux_series.index >= xlim[0])]
            ylim = set_fig_ylimits(ax=ax, flux_series=flux_in_plot, ylim=ylim)
            ax.set_xlim(xlim[0], xlim[1])
            ax.set_yscale(yscale)

            # The measurements. kw where dictates where the step happens -> "mid" for at the middle of the bin
            ax.step(time, flux_series.values, c="C0", where="mid")

            if erase is not None:
                ax.scatter(glitches.index, glitches.values, s=3, c='maroon')
                if diagnostics:
                    print("omitted values:")
                    print(glitches)

            # Onset time
            if onset_found:
                ax.axvline(x=onset_stats[-1], linewidth=1.5, color=color_dict["onset_time"], linestyle= '-', 
                            label="onset time")

                # Textbox indicating onset time
                onset_timestr = onset_stats[-1].strftime("%H:%M:%S")
                plabel = AnchoredText(f"Onset time: {onset_timestr} ", prop=dict(size=TXTBOX_SIZE), frameon=True, loc="lower right") #str(onset_stats[-1])[:19]
                plabel.patch.set_boxstyle("round, pad=0., rounding_size=0.2")
                plabel.patch.set_linewidth(2.0)
                ax.add_artist(plabel)

            # Background mean
            ax.axhline(y=onset_stats[0], linewidth=2, color=color_dict["bg_mean"], linestyle= "--", 
                        label=r"$\mu_{a}$ for background intensity")

            # Background mean + n*std
            ax.axhline(y=onset_stats[1], linewidth=2, color=color_dict["bg_mean"], linestyle= ':',
                        label=r"$\mu_{d}$ for background intensity")

            # Background shaded area
            if isinstance(background_range, BootstrapWindow):
                background_range.draw_background(ax=ax)
            else:
                ax.axvspan(xmin=background_start, xmax=background_end,
                            color=color_dict["bg"], label="Background", alpha=0.15)

            # Plot the earliest possible onset time, mean of medians and confidence intervals if they're found
            if show_stats:

                # Mean of median onsets accompanied by their confidence intervals
                if self.mean_of_medians_onset_acquired:

                    ax.axvline(x=self.mean_of_medians_onset, linewidth=1.5, color="magenta", linestyle='-',
                            label="mean of medians onset")

                    ax.axvspan(xmin=self.conf1_low, xmax=self.conf1_high, color="red", alpha=0.3, label="~68 % confidence")
                    ax.axvspan(xmin=self.conf2_low, xmax=self.conf2_high, color="blue", alpha=0.3, label="~95 % confidence")


            ax.set_xlabel(f"Time ({time[0].year})", fontsize=AXLABEL_FONTSIZE)
            ax.set_ylabel(self.unit, fontsize=AXLABEL_FONTSIZE)

            # Date tick locator and formatter
            ax.xaxis_date()
            set_standard_ticks(ax=ax)

            utc_dt_format1 = DateFormatter(f'%H:%M {NEWLINE}%m-%d')
            ax.xaxis.set_major_formatter(utc_dt_format1)

            # Setting the title
            if title is None:

                title = self._get_title(energy_str=en_channel_string, time_resolution=time_reso)
                # To be deleted in a later patch for redundancy
                # if viewing:
                #     ax.set_title(f"{spacecraft}/{self.sensor.upper()} ({viewing}) {en_channel_string} {self.s_identifier}\n{time_reso} data", fontsize=TITLE_FONTSIZE)
                # else:
                #     ax.set_title(f"{spacecraft}/{self.sensor.upper()} {en_channel_string} {self.s_identifier}\n{time_reso} data", fontsize=TITLE_FONTSIZE)

            ax.set_title(title, fontsize=TITLE_FONTSIZE)

            if diagnostics:
                ax.legend(loc="best", fontsize=LEGEND_SIZE)

            # Attach the figure to class attribute even if not saving the figure
            self.fig, self.ax = fig, ax

            # fig.savefig(fname=f"{savepath}{os.sep}{fname}",
            #                 facecolor="white", transparent=False, bbox_inches="tight")

            # Run additional diagnostic tools -> Add two subplots displaying the z-standardized intensity
            # time series (with k) and a heatmap displaying k as a function of bg mu and sigma.
            if diagnostics:

                # Create gridspec to align new plots
                gc = fig.add_gridspec(nrows=3, ncols=2, hspace=0.19)

                # Add new axes to the bottom row of the figure
                z_ax = fig.add_subplot(gc[1,0])
                c_ax = fig.add_subplot(gc[2,0])
                k_ax = fig.add_subplot(gc[1,1])

                # Plotting (k_contour returns the colorbar if axes are readily provided)
                self.z_score_plot(series=onset_stats[4], background=background_range, n_sigma=sigma_multiplier, 
                                  ax=z_ax, xlim=xlim, k_model=k_model, norm=norm)

                # Plotting the CUSUM function. onset_stats[5] == cusum, onset_stats[3] == h
                self.cusum_plot(cusum=onset_stats[5], h=onset_stats[3], background=background_range, ax=c_ax,
                                xlim=xlim)

                # Plotting the k-contour
                k_cb = background_range.k_contour(sigma_multiplier=sigma_multiplier, fig=fig, ax=k_ax, k_model=k_model)

                offset_down_z = 0.71
                zc_height_increment = 0.03
                # Moving z-score plot down and removing ticklabels
                z_pos = z_ax.get_position()
                new_z_pos = [z_pos.x0, z_pos.y0 - offset_down_z, z_pos.width, z_pos.height+zc_height_increment]
                z_ax.set_position(new_z_pos)
                z_ax.xaxis.set_tick_params(which="both", labelbottom=False, direction="in")

                offset_down_c = 0.75
                # Moving the cusum plot down
                c_pos = c_ax.get_position()
                new_c_pos = [c_pos.x0, c_pos.y0 - offset_down_c, c_pos.width, c_pos.height+zc_height_increment]
                c_ax.set_position(new_c_pos)
                c_ax.xaxis.set_major_formatter(utc_dt_format1)

                offset_down_k = 1.03
                k_height_increment = 0.35
                # Moving the colormap and colorbar down
                k_pos = k_ax.get_position()
                new_k_pos = [k_pos.x0, k_pos.y0 - offset_down_k, k_pos.width, k_pos.height+k_height_increment]
                k_ax.set_position(new_k_pos)
                bar_pos = k_cb.ax.get_position()
                new_bar_pos = [bar_pos.x0, bar_pos.y0 - offset_down_k, bar_pos.width+0.45, bar_pos.height+0.35]
                k_cb.ax.set_position(new_bar_pos)

            # Everything plotting-related has been done. The figure can be saved.
            if save:
                if savepath is None:
                    savepath = CURRENT_PATH

                # Use a default name for the figure if custom name was not provided. The
                # default name is generated here.
                if not isinstance(fname, str):

                    if spacecraft.lower() in ["bepicolombo", "bepi"]:
                        fname = f"{self.spacecraft}_{self.sensor}_side{viewing}_{self.species}_{channels}_onset.png"
                    elif viewing != "" and viewing is not None:
                        fname = f"{self.spacecraft}_{self.sensor}_{viewing.lower()}_{self.species}_{channels}_onset.png"
                    else:
                        fname = f"{self.spacecraft}_{self.sensor}_{self.species}_{channels}_onset.png"

                # Save the figure:
                save_figure(figure=fig, fname=fname, savepath=savepath)

            # Show the figure if it was created
            plt.show()

        # Assemble a dictionary of return values:
        onset_stats_dict = {}

        onset_stats_dict["bg_mu"] = onset_stats[0]
        onset_stats_dict["bg_mu_d"] = onset_stats[1]
        onset_stats_dict["k_parameter"] = onset_stats[2]
        onset_stats_dict["hastiness_threshold"] = onset_stats[3]
        onset_stats_dict["ints_norm"] = onset_stats[4]
        onset_stats_dict["cusum"] = onset_stats[5]
        onset_stats_dict["onset_time"] = onset_stats[6]
        onset_stats_dict["energy_string"] = en_channel_string

        return onset_stats_dict, flux_series


    def cusum_plot(self, cusum:np.ndarray, h:int, background:BootstrapWindow, ax:plt.Axes, xlim:tuple|list=None,
                   yscale:str="log") -> plt.Figure:
        """
        Plots the CUSUM function.

        Parameters:
        -----------
        cusum : {np.ndarray} The CUSUM function.
        h : {int} The hastiness threshold of CUSUM.
        background : {BootstrapWindow}
        ax : {plt.Axes}
        xlim : {tuple|list}
        yscale : {str}

        Returns:
        ------------
        fig : {plt.Figure}
        ax : {plt.Axes}
        """

        # Initialize the figure
        if ax is None:
            fig, ax = plt.subplots(figsize=(13,7))
            ax_provided = False
        else:
            ax_provided = True
        
        ax.set_yscale(yscale)
        ax.set_ylim(5e-1, 1.2*np.max(cusum))

        if isinstance(xlim, (list,tuple)):
            ax.set_xlim(pd.to_datetime(xlim[0]), pd.to_datetime(xlim[1]))

        ax.step(self.flux_series.index, cusum, color="maroon", label="CUSUM")
        ax.axhline(y=h, ls="--", color='k', label=f"h={h}")

        # Display the background on the plot
        ax.axvspan(xmin=background.start, xmax=background.end, color="#e41a1c", alpha=0.10)

        ax.set_title("CUSUM")
        ax.legend()

        if ax_provided:
            return None
    
        return (fig,ax)


    def z_score_plot(self, series:pd.Series, background:BootstrapWindow, n_sigma:int, ax:plt.Axes=None, yscale:str="log",
                     xlim:tuple|list=None, ylim:tuple|list=None, k_model:str=None,
                     norm:str='z') -> plt.Figure:
        """
        Plots the z-score of the event, i.e., the z-standardized intensity.
        Displays the mean, standard deviation and the k-parameter calculated from background params.

        Parameters:
        -----------
        series : {pd.Series} the time series to plot
        background : {BootstrapWindow}
        n_sigma : {int}
        ax : {plt.Axes}
        y_scale : {str}
        xlim : {tuple|list}
        ylim : {tuple|list}
        k_model : {Callable,str} Model that calculates k. Either a function with an identical signature
                    to the k_parameter() defined in this software, or the string 'legacy' for the old
                    definition of k.
        norm: {str} Either 'z' (default) or 'sigma'.

        Returns:
        -----------
        fig : {plt.Figure}
        ax : {plt.Axes}
        """

        # Uses the latest flux_series (bound to the object) and the chosen background
        # params (mu,sigma) to calculate z-standardized intensity
        mu = background.background_selection.mean()
        sigma = background.background_selection.std()

        if norm=='z':
            norm_mu = 0
            norm_sigma = 1
        else:
            norm_mu = mu/sigma
            norm_sigma = 1

        # Initialize the figure
        if ax is None:
            fig, ax = plt.subplots(figsize=(13,7))
            ax_provided = False
        else:
            ax_provided = True

        ax.set_yscale(yscale)

        if isinstance(xlim, (list,tuple)):
            ax.set_xlim(pd.to_datetime(xlim[0]), pd.to_datetime(xlim[1]))

        if isinstance(ylim, (list,tuple)):
            ax.set_ylim(ylim[0], ylim[1])

        ax.step(series.index, series.values, color="green", where="mid", zorder=1)
        ax.axhline(y=norm_mu, c="red", ls="--", label=r"$\mu$=0")
        ax.axhline(y=norm_sigma, c="red", ls=":", label=r"$\sigma$=1")

        # Display the background on the plot
        ax.axvspan(xmin=background.start, xmax=background.end, color="#e41a1c", alpha=0.10)

        # Calculate k and mark it as a horizontal dashed line
        if k_model is None:
            k_model = k_parameter
        elif k_model=="legacy":
            k_model = k_legacy
        else:
            # Do nothing. Hopefully the user knows what they're doing.
            pass
        current_k = k_model(mu=mu, sigma=sigma, sigma_multiplier=n_sigma)
        ax.axhline(y=current_k, c='k', ls="--", zorder=2, label=f"k={current_k:.2f}")

        if norm=='z':
            title_str = "z-standardized intensity"
        else:title_str = r"$\sigma$-standardized intensity"
        ax.set_title(title_str)
        ax.legend()

        if ax_provided:
            return None

        return (fig, ax)


    def final_onset_plot(self, channel, resample:str=None, xlim:tuple|list=None, ylim:tuple|list=None,
            show_background:bool=True, peak:bool=False,
            onset:str="mode", title:str=None, legend_loc:str="out", legend_side:str="right",
            savepath:str=None, save:bool=False, fname:str=None,
            return_figure:bool=False) -> dict:
        """
        Produces the 'final' plot that showcases the intensity time series, the onset time and its 
        confidence intervals and the background selection.

        By default:
            -plots the intensity in 1 min cadence, even for instruments with finer cadence.
            -sets the horizontal boundaries to +3/-5 hours from the onset time.
            -sets the vertical boundaries like cusum_onset() does (data_min/2, data_max*1.5).

        Parameters:
        ----------
        channel : {int,str} 
                    The channel identifier.
        resample : {str}, optional
                    Pandas-compatible time string to time-average the inetsnity time series.
        xlim : {tuple,list}, optional
                    A pair of datetime strings to set the horizontal boundaries of the plot.
        ylim : {tuple,list}, optional
                    A pair of floating point numbers to set the vertical boundaries of the plot.
        show_background : {bool}, optional
                    A switch to draw the background selection on the plot.
        peak : {bool}, optional
                    A switch to find the peak intensity in the plot, display it with a vertical line,
                    and add it's timestamp, value and (possible) time-averaging for which the peak intensity
                    was found for.
        onset : {str}, optional
                    A switch to choose either 'mode' or 'median' onset of the analysis as the onset to display.
        title : {str}, optional
                    A custom title.
        legend_loc : {str}, optional
                    The legend location, either 'in' or 'out'.
        legend_side : {str}, optional
                    If legend is 'in', chooses the side at which the legend is located. 'right' or 'left'.
                    Defaults to 'right'.
        savepath : {str}, optional
                    A path to save the figure.
        save : {bool}, optional
                    A switch to save the figure and a csv table containing analysis parameters. The parameters
                    are also ALWAYS returned as a dictionary.
        fname : {str}, optional
                    A custom name for the figure (and the corresponding csv) if saved.
        return_figure : {bool}
                    Returns the figure instead of the dictionary of event information.
        Returns:
        ---------
        event_dict : {dict} Contains 'onset_time', 'confidence_interval1', 
                            'confidence_interval2', sigma_multiplier, 
                            'background', 'limit_averaging', 'max_averaging', 
                            'peak_intensity', 'peak_time'
        """

        FRST_CONF_START = 2
        FRST_CONF_END = 3
        SCND_CONF_START = 4
        SCND_CONF_END = 5
        VALID_ONSET_OPTIONS = ("mode", "median")
        HMD_FORMAT = DateFormatter(f"%H:%M{NEWLINE}%d")
        ONSETTIME_FORMAT = "%Y-%m-%d\n%H:%M:%S"
        HMINSEC_FORMAT = "%H:%M:%S"
        DEFAULT_MINUS_OFFSET_HOURS = 5
        DEFAULT_PLUS_ONSET_HOURS = 3

        if onset not in VALID_ONSET_OPTIONS:
            raise ValueError(f"parameter onset=={onset} when valid options are {VALID_ONSET_OPTIONS}.")
        onset_idx = 0 if onset=="mode" else 1 

        # Choose the onset time (mode/median) and confidence intervals
        onset_time = self.onset_statistics[channel][onset_idx]
        conf_interval1_start, conf_interval1_end = self.onset_statistics[channel][FRST_CONF_START], self.onset_statistics[channel][FRST_CONF_END] 
        conf_interval2_start, conf_interval2_end = self.onset_statistics[channel][SCND_CONF_START], self.onset_statistics[channel][SCND_CONF_END]

        event_dict = {
            "onset_time" : onset_time,
            "confidence_interval1" : [conf_interval1_start, conf_interval1_end],
            "confidence_interval2" : [conf_interval2_start, conf_interval2_end],
            "sigma_multiplier" : self.sigma_multiplier,
            "background" : [self.background.start, self.background.end],
            "limit_averaging" : pd.Timedelta(minutes=self.background.max_recommended_reso)
        }

        # Choose either custom data or standard to plot
        if self.custom_data:
            series, en_channel_string = self.data[channel], channel
        # Standard (SEPpy) data:
        else:
            series, en_channel_string = self.choose_flux_series(channels=[channel], viewing=self.viewing)

        # Resample the data if requested
        if isinstance(resample,str):
            series = util.resample_df(series, resample)

        # Resample data to 1 min even if not requested if fine cadence
        if resample is None and self.spacecraft in FINE_CADENCE_SC:
            series = util.resample_df(series, "1 min")

        # Define the boundaries of the plot. First setting is valid only if xlim was not given (not a tuple nor a list)
        # and if onset_time is a legit Timestamp (not a pd.NaT).
        if not isinstance(xlim, (tuple,list)):

            if not isinstance(onset_time, pd._libs.tslibs.nattype.NaTType):
                xlim = (onset_time - pd.Timedelta(hours=DEFAULT_MINUS_OFFSET_HOURS), onset_time + pd.Timedelta(hours=DEFAULT_PLUS_ONSET_HOURS))
            else:
                xlim = (series.index[0], series.index[-1])
        else:
            xlim = (pd.to_datetime(xlim[0]), pd.to_datetime(xlim[1]))

        # Time resolution of the data is for the title
        time_resolution = get_time_reso(series=series)

        # After resampling, if peak is to be found it's found here:
        if peak:

            # First apply a selection to the data, to only consider the part of data that exists within
            # the figure boundaries
            intensity_in_plot = series.loc[(series.index>=xlim[0])&(series.index<=xlim[1])]
            peak_intensity = intensity_in_plot.max()
            peak_int_time = intensity_in_plot.idxmax()

        # This is the maximum time-averaging that was used in the hybrid method to find the onset
        max_avg_time = self.max_avg_times[channel]
        event_dict["max_averaging"] = max_avg_time

        # Creating the plot, and all the plotting related code ->
        fig, ax = plt.subplots(figsize=STANDARD_FIGSIZE)

        # Set the x-axis settings
        ax.xaxis_date()
        ax.set_xlabel(f"Time ({onset_time.date()})", fontsize=AXLABEL_FONTSIZE)
        ax.set_xlim(xlim)
        ax.xaxis.set_major_formatter(HMD_FORMAT)

        # Modify the ticks such that the day is removed from all but the 
        # first and midnight ticks
        midnight_format_ticks(ax=ax)

        # The y-axis settings:
        ax.set_yscale("log")
        ax.set_ylabel(self.unit, fontsize=AXLABEL_FONTSIZE)
        _ = set_fig_ylimits(ax=ax, flux_series=series, ylim=ylim)

        # Intensity
        ax.step(series.index, series.values, color="tab:blue",   where="mid")

        # Onset time and confidence intervals:
        if not isinstance(onset_time, pd._libs.tslibs.nattype.NaTType):
            ax.axvline(x=onset_time, color="red", zorder=5, label=f"Onset time:{NEWLINE}{onset_time.strftime(ONSETTIME_FORMAT)}")
            ax.axvspan(xmin=conf_interval1_start, xmax=conf_interval1_end, 
                        color=COLOR_SCHEME["1-sigma"], zorder=2, alpha=.3,
                        label=f"~68 % confidence{NEWLINE}{conf_interval1_start.strftime(HMINSEC_FORMAT)}-{conf_interval1_end.strftime(HMINSEC_FORMAT)}")
            ax.axvspan(xmin=conf_interval2_start, xmax=conf_interval2_end, 
                        color=COLOR_SCHEME["2-sigma"], zorder=1, alpha=.3,
                        label=f"~95 % confidence{NEWLINE}{conf_interval2_start.strftime(HMINSEC_FORMAT)}-{conf_interval2_end.strftime(HMINSEC_FORMAT)}")

        # Shade the background only if asked to and if it overlaps with the plot boundaries
        if show_background:
            self.background.draw_background(ax=ax)
            if self.background.end < pd.to_datetime(xlim[0]):
                # Getting the last patch that was created (background.axvspan) and setting its label to
                # start with an underscore "_" makes it not appear on the legend
                background_shading = ax.patches[-1]
                background_shading.set_label("_Background")

        # Create a textbox that shows the maximum averaging time
        max_averaging_reso_textbox(max_avg_time, legend_loc=legend_loc, ax=ax)

        # If peak was found, draw it on the plot and add it to the legend and textbox
        if peak:
            peak_int_label = f"Peak Intensity:{NEWLINE}{peak_intensity:.2f}{NEWLINE}{peak_int_time.strftime(ONSETTIME_FORMAT)}"
            ax.axvline(x=peak_int_time, color="navy", lw=2., label=peak_int_label)

            # Add peak info to the dictionary
            event_dict["peak_intensity"] = peak_intensity
            event_dict["peak_time"] = peak_int_time

        # Finalize the plot with tickmarks, title and legend
        set_standard_ticks(ax=ax)
        set_legend(ax=ax, legend_loc=legend_loc, fontsize=LEGEND_SIZE, legend_side=legend_side)

        if title is None:
            title =self._get_title(energy_str=en_channel_string, time_resolution=time_resolution)
        ax.set_title(title, fontsize=TITLE_FONTSIZE)

        # Saving the figure
        if save:
            # Custom savepath if provided
            if savepath is None:
                savepath = CURRENT_PATH

            # Generate a name for the fig IF custom name was not provided
            if not isinstance(fname,str):
                onset_yyyymmdd_str = onset_time.strftime("%Y%m%d")
                if self.spacecraft.lower() in ["bepicolombo", "bepi"]:
                    fname = f"{self.spacecraft}_{self.sensor}_side{self.viewing}_{self.species}_{channel}_{onset_yyyymmdd_str}"
                elif self.viewing is not None:
                    fname = f"{self.spacecraft}_{self.sensor}_{self.viewing.lower()}_{self.species}_{channel}_{onset_yyyymmdd_str}"
                else:
                    fname = f"{self.spacecraft}_{self.sensor}_{self.species}_{channel}_{onset_yyyymmdd_str}"

                # If peak was found, add to the fname
                if peak:
                    fname += "peak"
                # If resampling was applied, add it to the end of the fname
                if resample is not None:
                    fname += resample

                # Finally add fileformat to the figure name
                fname += ".png"

            # Save the figure:
            save_figure(figure=fig, fname=fname, savepath=savepath)

            # ...and the csv:
            figformat = fname.split('.')[-1]
            csvname = fname.replace(figformat,"csv")
            event_params_to_csv(event_params=event_dict, filename=f"{csvname}")

        # Finally show the plot
        plt.show()

        if return_figure:
            return (fig, ax)
        return event_dict


    def plot_all_channels(self, viewing:str=None, resample:str=None, omit:list=None, cmap:str="twilight_shifted", xlim=None, title:str=None,
                          save=False, savepath=None):
        """
        Plots by default all the energy channels of the currently chosen instrument in the same time series plot.
        
        Parameters:
        -----------
        viewing : {str}, optional
                    The viewing direction of the instrument. If not provided then use the currently 
                    active viewing direction from self.viewing.
        resample : {str}, optional
                    The time-averaging for the time series. Use a pandas-compatible time string, e.g., '1min' or '600s'
        omit : {list, tuple}, optional
                    A list or a tuple of the channels to forgo plotting.
        cmap : {str}, optional
                    The colormap used to plot the channels.
        xlim : {tuple or list with __len__ == 2}, optional
                    The x-axis limits of the figure.
        title : {str}, optional
                    A custom title for the figure.
        save : {bool}, optional
                    Boolean save switch, default False.
        savepath : {str}, optional
                    Either relative or absolute path leading to the directory where the plot should be saved to.
        
        Returns:
        -----------
        fig, ax : The figure and the axis objects.
        """

        # Choose viewing
        viewing = viewing if viewing else self.viewing

        # Collect all channels to a list
        all_channels = self.get_all_channels()

        # Choose which channels to plot, by default plot everything
        plotted_channels = all_channels if not omit else list(set(all_channels) - set(omit))

        # Get a colormap for the different channels.
        colors = plt.get_cmap(cmap, len(plotted_channels))

        # Initializing the figure and axes
        fig, ax = plt.subplots(figsize=STANDARD_FIGSIZE)

        # Before starting the plot, save the original rcParam options and update to new ones
        # original_rcparams = self.save_and_update_rcparams("onset_tool")
        # This seemed to sometimes get figure visual parameters messed up, such as fontsize of the title and figure size

        # Plotting loop
        for i, channel in enumerate(plotted_channels):

            # Getting a time series from the object's dataframe
            intensity, en_channel_string = self.choose_flux_series(channels=[int(channel)], viewing=viewing)

            if isinstance(en_channel_string,(list, np.ndarray)):
                en_channel_string = en_channel_string[0]

            # If time-averaging, do it here before plotting
            if resample:
                intensity = util.resample_df(intensity, resample)

            # Plotting a channel. kw where=="mid" to ensure that step happens in the middle of the time bin
            ax.step(intensity.index, intensity.values, where="mid", label=en_channel_string, color=colors(i))


        # Setting the x-axis limits
        if not xlim:
            xlim = [intensity.index[0], intensity.index[-1]]
        else:

            # Check that xlim makes sense
            if xlim[0] == xlim[1]:
                raise Exception("xlim[0] and xlim[1] cannot be the same time!")

            xlim[0], xlim[1] = pd.to_datetime(xlim[0]), pd.to_datetime(xlim[1])
        
        ax.set_xlim(xlim)

        ax.set_yscale("log")

        ax.set_xlabel("Time", fontsize=20)
        ax.set_ylabel(f"Intensity [1/(cm^2 sr s MeV)]", fontsize=20)

        # Tickmarks, their size etc...
        ax.tick_params(which='major', length=6, width=2, labelsize=17)
        ax.tick_params(which='minor', length=5, width=1.4)

        # Date tick locator and formatter
        ax.xaxis_date()
        #ax.xaxis.set_major_locator(ticker.AutoLocator()) # ticker.MaxNLocator(9)
        utc_dt_format1 = DateFormatter(f'%H:%M {NEWLINE}%Y-%m-%d')
        ax.xaxis.set_major_formatter(utc_dt_format1)

        ax.legend(loc=3, bbox_to_anchor=(1.0, 0.01), prop={'size': 16})

        # Setting the title
        if title is None:

            # Choosing the particle species identifier for the title
            s_identifier = self._get_species_identifier()

            if viewing:
                ax.set_title(f"{self.spacecraft.upper()}/{self.sensor.upper()} {s_identifier}{NEWLINE}"
                        f"{resample} averaging, viewing: "
                        f"{viewing.upper()}", fontsize=TITLE_FONTSIZE)
            else:
                ax.set_title(f"{self.spacecraft.upper()}/{self.sensor.upper()} {s_identifier}{NEWLINE}"
                        f"{resample} averaging", fontsize=TITLE_FONTSIZE)
        
        else:
            ax.set_title(title, fontsize=TITLE_FONTSIZE)

        if save:
            if savepath is None:
                savepath = CURRENT_PATH

            if self.spacecraft.lower() in ['bepicolombo', 'bepi']:
                fig.savefig(f"{savepath}{os.sep}{self.spacecraft}_{self.sensor}_side{viewing}_{self.species}_all_channels.png", transparent=False,
                        facecolor='white', bbox_inches='tight')
            elif viewing != "" and viewing:
                fig.savefig(f"{savepath}{os.sep}{self.spacecraft}_{self.sensor}_{viewing.lower()}_{self.species}_all_channels.png", transparent=False,
                        facecolor='white', bbox_inches='tight')
            else:
                fig.savefig(f"{savepath}{os.sep}{self.spacecraft}_{self.sensor}_{self.species}_all_channels.png", transparent=False,
                        facecolor='white', bbox_inches='tight')

        plt.show()

        return fig, ax


    def statistic_onset(self, channels, Window, viewing:str, resample:str=None, erase:tuple=None, sample_size:float=None, cusum_minutes:int=None, 
                        small_windows:str=None, offset_origins:bool=True, detrend=True, sigma_multiplier=2,
                        k_model=None):
        """
        This method looks at a particular averaging window with length <windowlen>, and draws from it
        points <n_bstraps> times. From these <n_bstraps> different distributions of measurements, it 
        calculates the mean and standard deviation for each distribution. Using the acquired distribution of
        different means and standard deviations, it produces a distribution of expected onset times. Finally
        the method moves the averaging window forward if windows has n_shifts>1.
        
        The procedure described above is repeated <n_windows> times, yielding <n_windows>*<n_bstraps> onset times.
        
        Parameters:
        -----------
        channels : int or list
                    Channel or channels that the onset is searched from
        Window: BootstrapWindow object
                    A BootstrapWindow object that holds information about averaging window parameters
        viewing : str
                    The viewing direction of the instrument
        resample : str
                    A pandas-compatible time string representing the averaging of data
        erase : tuple(float, str)
                    The maximum acceptable intensity value and the timestamp at which larger values are accepted
        sample_size : {float}, optional
                    The proportion of the ensemble of data points inside a window that are drawn for the bootstrapping method
        cusum_minutes : {int}, optional 
                    An integer stating the amount of minutes that threshold exceeding intensity must consistently be observed 
                    before identifying an onset. If not provided, use a set of 4,8, 16 and 32 times the current data resolution
        small_windows : str, default None
                    Choose either 'random' or 'equal' placement of the subwindows. Randomly placed windows can overlap,
                    while equally placed windows start where the previous ended.
        offset_origins : {bool}, default True
                    Boolean switch to control if onset times can be found on a continuous scale or only at intervals allowed by the
                    resolution of the data. By default origin is being offset, i.e., onset can be found at 1-minute resolution
                    regardless of the integration time.
        detrend : bool, default True
                    Controls if the onset times are 'de-trended', i.e., shifted forward in time by 0.5*time_resolution - 30s. Does not 
                    apply to 1min data.
        sigma_multiplier : {int, float} default 2
                    The multiplier n for the $\\mu_{d}$ variable in the CUSUM method.
        k_model : {Callable|str} Chooses the model for the k-parameter from the calc_utilities library in PyOnset.
        Returns:
        --------
        mean_onset: a timestamp indicating the mean of all onset times found
        std_onset: the standard deviation of onset times
        most_common_val: the most common onset time found
        percent_mode: the prevalence of the most common onset time (fraction of all onset times)
        onset_list: a list containing all onset times in the order they were calculated by this function
        """
        # Bootstrapping:
        # https://towardsdatascience.com/calculating-confidence-interval-with-bootstrapping-872c657c058d

        # Channels are considered a list in the data loader, that's why this check is done here
        self.last_used_channel = channels
        if isinstance(channels,(int,str)):
            channels = [channels]


        self.current_channel_id = channels[0]

        # Do not search for onset earlier than this point
        big_window_end = Window.end

        # Before going to pick the correct flux_series, check that viewing is reasonable.
        if viewing is not None and not self.check_viewing(returns=True):
            raise ValueError("Invalid viewing direction!")

        # Choose the right intensity time series according to channel and viewing direction.
        # Also remember which channel was examined most recently.
        if not self.custom_data:
            flux_series, self.recently_examined_channel_str = self.choose_flux_series(channels, viewing)
        else:
            flux_series, self.recently_examined_channel_str = self.data[channels[0]], channels[0]

        # Create a timedelta representation of resample, for convenience
        resample_td = pd.Timedelta(resample)
        # By default there will be a list containing timeseries indices of varying origins of offset
        if offset_origins and resample:

            # Separate the unit of resample and its numerical value
            resample_unit_str = find_biggest_nonzero_unit(timedelta=resample_td)
            resample_value = int(resample[:-3]) if resample_unit_str=="min" else int(resample[:-1])
            if resample[-3:] == "min":

                # This integer is one larger than the offset will ever go
                data_res = int(resample_value)

                # There will be a set amount of varying offsets with linear intervals to the time indices of flux_series.
                # offsets are plain integers
                offsets = np.arange(data_res)

                # Using the number of offsets, compile a list of copies of flux_series
                list_of_series = [flux_series if i==0 else flux_series.copy() for i in offsets]

                # Produces a list of indices of varying offsets. The first one in the list has offset=0, which are 
                # just the original indices. These indices will replace the indices of the series in the list.
                list_of_indices = [flux_series.index + to_offset(f"{offset}{resample_unit_str}") for offset in offsets]

                # These are a random numbers to access any of the indices in the list of indices.
                # Calculated beforehand to save computation time.
                random_choices = np.random.randint(data_res, size=Window.bootstraps)

            else:
                list_of_series = [flux_series]
                list_of_indices = [flux_series.index]
                random_choices = np.zeros(Window.bootstraps, dtype=int)

        # If we don't offset origin of timeseries, then just init list_of_series, list_of_indices and random_choises
        # with default values for a single unaltered flux_series
        else:

            list_of_series = [flux_series]
            list_of_indices = [flux_series.index]
            random_choices = np.zeros(Window.bootstraps, dtype=int)


        # Replace the indices of the copies of the series. Each new series will have its indices shifted
        # forward by one minute (in the current version).
        # After the possible shifting of indices, first erase possible erroneous data, only after 
        # resample it if need be.
        for i, series in enumerate(list_of_series):

            # Shifting of indices forward
            series.index = list_of_indices[i]

            # Erasing erroneous values
            if erase:
                series, _ = erase_glitches(series, erase[0], erase[1])

            # Resampling of data to a differrent resolution
            if resample:
                series = util.resample_df(series, resample)

            # Finally placing the series back to the list at the correct position
            list_of_series[i] = series


        # These apply regardless of which series we choose, as they all have the same resolution and hence cusum_window
        if not self.custom_data:
            # if not resample:
            #     time_reso = f"{int(self.get_minimum_cadence().seconds/60)} min" if self.get_minimum_cadence().seconds > 59 else f"{int(self.get_minimum_cadence().seconds)} s"
            # else:
            #     time_reso = get_time_reso(list_of_series[0])
            # At the current stage there should be no reason to resort to pre-defined minimum cadences. This is why
            # on 2025-11-13 I'm commenting the above part out. get_time_reso() will return the most prevalent 
            # time differential between consecutive data points.
            time_reso = get_time_reso(list_of_series[0])

        # in case of custom data: 
        else:
            time_reso = get_time_reso(list_of_series[0])
            if not cusum_minutes:
                raise Exception("Must provide a value for 'cusum_minutes' with custom data!")

        # If cusum_window was not explicitly stated, use a set of multiples of the time resolution as 
        # a set of cusum_windows
        if cusum_minutes:
            cusum_windows = [calculate_cusum_window(time_reso, cusum_minutes)]
        else:
            if time_reso[-3:]=="min":
                cusum_minutes_list = [c*int(time_reso[:-3]) for c in CUSUM_WINDOW_RESOLUTION_MULTIPLIERS]
            else:
                # For now just go with a fixed number of minutes if resolution is less than a minute
                cusum_minutes_list = [c for c in CUSUM_WINDOW_RESOLUTION_MULTIPLIERS]
                # cusum_minutes_list = [int(c*int(time_reso[:-1])/60) for c in cusum_window_resolution_multipliers]
            cusum_windows = [calculate_cusum_window(time_reso, cusum_minutes) for cusum_minutes in cusum_minutes_list]

        # Initialize necessary lists to collect data about onset time and the corresponding background parameters
        onset_list = []
        mus_list = []
        sigmas_list = []

        # Small windows will initialize a set of randomly placed smaller windows inside the chosen (input) 
        # BootstrapWindow object. The small windows will all have n_shifts=0, and bootstraps equal to the
        # amount the original Window had.
        if small_windows:

            # Before doing anything, check that small_windows is the type it is meant to be
            if small_windows not in ("random", "equal"):
                raise Exception(f"small_windows = {small_windows} is not a valid input! Choose either 'random' or 'equal'.")

            # First check that there are shifts in the Window, otherwise it makes no sense to even
            # call subwindows()
            if Window.n_shifts < 1:
                raise Exception(f"The number of small windows can't be {Window.n_shifts}")

            # Produce a list of little windows to loop over and apply bootstrapping
            windowlist = subwindows(Window=Window, window_placement=small_windows)

            # Set the class-attached list to an empty list, in order to stop new windows accumulating there every time
            # this method is run. This is essentially flushing the list clean.
            self.list_of_bootstrap_windows = []
            self.subwindows_plotted = False

        # If no small windows, just loop over one window
        else:
            windowlist = [Window]

        # Controls if a warning is printed in the event the background window is less than 100 data points. Hard coded to False for now
        prints_warning = False

        # Nested loops :(
        # But in reality either the middle or the outer loop is only run once
        for window in windowlist:

            # Shifts here is either the times one big background window is shifted towards the onset of the event,
            # or it is the amount of small little windows inside the larger background window
            shifts = window.n_shifts if window.n_shifts > 0 else 0

            for _ in range(shifts+1):

                # Init arrays to collect used background parameters
                mus, sigmas = np.array([]), np.array([])

                # Loop through the mus and sigmas of the current window, and find onsets 
                for j in range(window.bootstraps):

                    # Pick a number from random_choices to access a copy of flux_series in the list_of_series
                    # In the case we don't offset any of the series, there will be only one instance of flux_series in the
                    # list, and random_choices will be an array of only zeroes
                    chosen_series = list_of_series[random_choices[j]]

                    # Calculate background parameters (mean and std), and append them to their respective arrays
                    mu, sigma = sample_mean_and_std(window.start, window.end, chosen_series, sample_size=sample_size, prints_warning=prints_warning)
                    mus = np.append(mus, mu)
                    sigmas = np.append(sigmas,sigma)

                    # Choose the cusum_window to use for this iteration of the run
                    cusum_window = cusum_windows[j%len(cusum_windows)]

                    # Find an onset and save it into onset_series. Use the chosen series from the list of series
                    onset_i = onset_determination([mu, sigma], chosen_series, cusum_window, big_window_end, sigma_multiplier=sigma_multiplier,
                                                  k_model=k_model)
                    onset_list.append(onset_i[-1])


                # Append the arrays to the list of all bootstrapped background params
                mus_list.append(mus)
                sigmas_list.append(sigmas)

                # Finally move the averaging window forward one hour before repeating, or in the case of small_windows,
                # add the window to the class list of windows 
                if not small_windows:
                    window.move_window(hours=1)
                else:
                    self.add_bootstrap_window(window=window)

        # After exiting the loop, return the background window to its initialized state to not cause unintended effects later
        Window.load_state(which="init")

        # Sort the list of onsets to chronological order
        onset_list.sort()

        # If de-trending is to be done, it must be done here. De-trending can only be applied if the data was resampled
        if detrend and resample:
            rs_final_index = -3 if resample[-3:] == "min" else -1
            dt_shift = f"{int(30*int(resample[:rs_final_index]))+30}s"
            onset_list = detrend_onsets(onset_list, shift=dt_shift)

        # onset_series needs to be converted to DatetimeIndex, so that the elements' mean can be calculated
        onset_indices = pd.DatetimeIndex(onset_list)
        mean_onset = onset_indices.mean()
        std_onset = np.std(onset_indices)

        # unique_vals() returns a list of the unique appearances of the onsets and their respective counts
        unique_onsets, counts = unique_vals(onset_indices)

        # First calculate the relative abundances of each unique onset
        percent_modes = np.array([count/np.nansum(counts) for count in counts])

        # The final entry of counts is the number of NaTs in the input onset_indices. Hence, it doesn't make sense
        # to keep the redundant percentage aboard. Omit the last entry to avoid errors later. No information is 
        # lost here in doing this.
        percent_modes = percent_modes[:-1]

        # Sort the percentages and unique onsets in ascending order
        percent_indices = percent_modes.argsort() # numpy array.argsort() returns a list of indices of values in ascending order

        # If absolutely no onset was found, then percent_indices will be an empty list
        if len(percent_indices) == 0:

            mean_onset = pd.NaT
            median_onset = pd.NaT
            std_onset = pd.NaT
            most_common_val = (pd.NaT, np.nan)
            onset_list = [pd.NaT]
            unique_onsets = [pd.NaT]
            confidence_intervals = [(pd.NaT, pd.NaT), (pd.NaT, pd.NaT)]

        # Here the normal case where onsets are found is treated
        else:

            unique_onsets = unique_onsets[percent_indices]
            percent_modes = percent_modes[percent_indices]

            # Unique onsets hold tuples of onset-percentage pairs in ascending order (= least likely onset first and most likely last)
            unique_onsets = np.array([(onset, percent_modes[i]) for i, onset in enumerate(unique_onsets)])

            # The most "popular" onset time and its mode are the last entries in the arrays
            most_common_val = (unique_onsets[-1,0], unique_onsets[-1,1])

            # The median onset is the middlemost onset in the distribution
            median_onset =  datetime_nanmedian(onset_list)

            # Also calculate 1-sigma and 2-sigma confidence intervals for the onset distribution
            # The method here also check the validity of the confidence intervals, which is thy it takes as an input the time resolution used
            confidence_intervals = self.get_distribution_percentiles(onset_list=onset_list, percentiles=[(15.89,84.1), (2.3,97.7)], time_reso=time_reso)

        # Attach the statistics to class attributes for easier handling. The first entry of list_of_series is the 
        # original flux_series without shifting any indices
        self.flux_series = list_of_series[0]
        bootstrap_onset_statistics = {
                                    "mean_onset" : mean_onset,
                                    "median_onset" : median_onset,
                                    "std_onset" : std_onset,
                                    "most_likely_onset" : most_common_val,
                                    "onset_list" : onset_list,
                                    "unique_onsets" : unique_onsets,
                                    "1-sigma_confidence_interval" : confidence_intervals[0],
                                    "2-sigma_confidence_interval" : confidence_intervals[1]
                                    }

        mu_and_sigma_distributions = {
                                    "mus_list" : mus_list,
                                    "sigmas_list" : sigmas_list
                                    }

        self.bootstrap_onset_statistics = bootstrap_onset_statistics
        self.mu_and_sigma_distributions = mu_and_sigma_distributions

        return bootstrap_onset_statistics, mu_and_sigma_distributions


    def show_onset_distribution(self, show_background=True, ylim=None, xlim=None, returns=False):
        """
        Displays all the unique onsets found with statistic_onset() -method in a single plot. The mode onset, that is
        the most common out of the distribution, will be shown as a solid line. All other unique onset times are shown
        in a dashed line, with the shade of their color indicating their relative frequency in the distribution.

        Note that statistic_onset() has to be run before this method. Otherwise KeyError will be raised.

        show_background : {bool}, optional
                            Boolean switch to show the background used in the plot.
        
        ylim : {list,tuple}, optional

        xlim : {list,tuple}, optional

        returns : {bool}, optional 
        """

        rcParams["font.size"] = 20

        flux_series = self.flux_series
        most_likely_onset = self.bootstrap_onset_statistics["most_likely_onset"]
        onsets = self.bootstrap_onset_statistics["unique_onsets"]

        # Create a colormap for different shades of red for the onsets. Create the map with the amount of onsets+2, so that
        # the first and the final color are left unused. They are basically white and a very very dark red.
        cmap = plt.get_cmap("Reds", len(onsets)+2)

        # Creating the figure and axes
        fig, ax = plt.subplots(figsize=STANDARD_FIGSIZE)

        # x- and y-axis settings
        ylim = set_fig_ylimits(ax=ax, flux_series=flux_series, ylim=ylim)

        ax.set_yscale("log")
        ax.set_ylabel("Intensity [1/(cm^2 sr s MeV)]")

        if not xlim:
            xlim = (flux_series.index[0], flux_series.index[-1])
        ax.set_xlim(xlim)

        set_standard_ticks(ax=ax)
        ax.xaxis_date()
        ax.xaxis.set_major_locator(ticker.AutoLocator())
        utc_dt_format1 = DateFormatter(f'%H:%M {NEWLINE}%m-%d')
        ax.xaxis.set_major_formatter(utc_dt_format1)


        # Plots the intensity data. Keyword 'where' dictates where the step happens. Set 'mid' for the middle of the bin instead of the start.
        ax.step(flux_series.index, flux_series.values, where="mid")

        # Draws the background on the plot
        if show_background:
            self.background.draw_background(ax=ax)

        # Marking the onsets as vertical lines of varying shades of red
        for i, onset in enumerate(onsets):

            # Not the most likely onset
            if onset[0] != most_likely_onset[0]:
                linestyle = (5, (10, 3)) if onset[1] > 0.2 else (0, (5, 5))
                ax.axvline(x=onset[0], c=cmap(i+1), ls=linestyle, label=f"{str(onset[0])[11:19]} ({np.round(onset[1]*100,2):.2f} %)")

            # The most likely onset
            # I'm accessing the i+1th color value in the map because I never want the first value (that is white in the standard Reds colormap)
            else:
                ax.axvline(x=onset[0], c=cmap(i+1), label=f"{str(onset[0])[11:19]} ({np.round(onset[1]*100,2):.2f} %)") 

        # Legend and title for the figure
        ax.legend(loc=10, bbox_to_anchor=(1.0, 0.95), prop={'size': 12})
        ax.set_title("Onset distribution", fontsize=TITLE_FONTSIZE)

        plt.show()

        if returns:
            return fig, ax


    def show_onset_statistics(self, percentiles=[(15.89,84.1),(2.3,97.7)], show_background=True, xlim=None, ylim=None):
        """
        Shows the median, mode, mean and confidence intervals for the distribution of onsets got from statistic_onset()

        Parameters:
        -----------
        percentiles : {tuple, list}, optional
                    Choose which kind of percentiles of the distribution to display
        show_bacground : {bool}, optional
                    Boolean switch to show the background on the plot.
        """

        # Take the 1-std and 2-std limits from a normal distribution (default)
        sigma1, sigma2 = percentiles[0], percentiles[1]

        # Collect the 1-sigma and 2-sigma confidence intervals out of the onset distribution
        confidence_intervals = self.get_distribution_percentiles(onset_list = self.bootstrap_onset_statistics["onset_list"], percentiles=[sigma1,sigma2])

        # Collect the median, mode and mean onsets from the distribution
        onset_median = self.bootstrap_onset_statistics["median_onset"]
        onset_mode = self.bootstrap_onset_statistics["most_likely_onset"][0]
        onset_mean = self.bootstrap_onset_statistics["mean_onset"]

        # This is just for plotting with the latest resolution used
        flux = self.flux_series

        # Plot commands and settings:
        fig, ax = plt.subplots(figsize=STANDARD_FIGSIZE)

        ax.step(flux.index, flux.values, where="mid")

        # Draw the background
        if show_background:
            self.background.draw_background(ax=ax)

        # Vertical lines for the median, mode and mean of the distributions
        ax.axvline(onset_median, c="red", label="median")
        ax.axvline(onset_mode, c="blue", label="mode")
        ax.axvline(onset_mean, c="purple", label="mean")

        # 1-sigma uncertainty shading
        ax.axvspan(xmin=confidence_intervals[0][0], xmax=confidence_intervals[0][1], color="red", alpha=0.3)

        #2-sigma uncertainty shading
        ax.axvspan(xmin=confidence_intervals[1][0], xmax=confidence_intervals[1][1], color="blue", alpha=0.3)

        # Figure settings
        ax.set_yscale("log")
        ax.set_ylabel("Intensity", fontsize=AXLABEL_FONTSIZE)
        ax.set_xlabel("Time", fontsize=AXLABEL_FONTSIZE)

        ylim = set_fig_ylimits(ax=ax, flux_series=flux, ylim=ylim)

        # x-axis settings
        if not xlim:
            xlim = (flux.index[0], flux.index[-1])
        ax.set_xlim(xlim)

        set_standard_ticks(ax=ax)
        ax.xaxis_date()
        ax.xaxis.set_major_locator(ticker.AutoLocator())
        utc_dt_format1 = DateFormatter(f'%H:%M {NEWLINE}%m-%d')
        ax.xaxis.set_major_formatter(utc_dt_format1)

        ax.legend()
        plt.show()


    def choose_flux_series(self, channels, viewing) -> tuple:
        """
        Method to choose the correct Series from a dataframe.

        Parameters:
        -----------
        channels : list
        viewing : str

        Returns:
        -----------
        flux_series : pd.Series
        en_channel_string : dict
        """

        self.choose_data(viewing)
        
        if (self.spacecraft[:2].lower() == 'st' and self.sensor == 'sept') \
                or (self.spacecraft.lower() == 'psp' and self.sensor.startswith('isois')) \
                or (self.spacecraft.lower() == 'solo' and self.sensor == 'ept') \
                or (self.spacecraft.lower() == 'solo' and self.sensor == 'het') \
                or (self.spacecraft.lower() == 'wind' and self.sensor == '3dp') \
                or (self.spacecraft.lower() == 'bepi'):
            self.viewing_used = viewing
            self.choose_data(viewing)
        elif (self.spacecraft[:2].lower() == 'st' and self.sensor == 'het'):
            self.viewing_used = ''
        elif (self.spacecraft.lower() == 'soho' and self.sensor == 'erne'):
            self.viewing_used = ''
        elif (self.spacecraft.lower() == 'soho' and self.sensor == 'ephin'):
            self.viewing_used = ''

        if self.spacecraft == 'solo':

            if self.sensor == 'het':

                if self.species in ['p', 'i']:

                    df_flux, en_channel_string =\
                        self.calc_av_en_flux_HET(self.current_df_i,
                                                 self.current_energies,
                                                 channels)
                elif self.species == 'e':

                    df_flux, en_channel_string =\
                        self.calc_av_en_flux_HET(self.current_df_e,
                                                 self.current_energies,
                                                 channels)

            elif self.sensor == 'ept':

                if self.species in ['p', 'i']:

                    df_flux, en_channel_string =\
                        self.calc_av_en_flux_EPT(self.current_df_i,
                                                 self.current_energies,
                                                 channels)
                elif self.species == 'e':

                    df_flux, en_channel_string =\
                        self.calc_av_en_flux_EPT(self.current_df_e,
                                                 self.current_energies,
                                                 channels)

            else:
                invalid_sensor_msg = "Invalid sensor!"
                raise Exception(invalid_sensor_msg)

        if self.spacecraft[:2] == 'st':

            # Super ugly implementation, but easiest to just wrap both sept and het calculators
            # in try block. KeyError is caused by an invalid channel choice.
            try:

                if self.sensor == 'het':

                    if self.species in ['p', 'i']:

                        df_flux, en_channel_string =\
                            calc_av_en_flux_ST_HET(self.current_df_i,
                                                   self.current_energies['channels_dict_df_p'],
                                                   channels,
                                                   species='p')
                    elif self.species == 'e':

                        df_flux, en_channel_string =\
                            calc_av_en_flux_ST_HET(self.current_df_e,
                                                   self.current_energies['channels_dict_df_e'],
                                                   channels,
                                                   species='e')

                elif self.sensor == 'sept':

                    if self.species in ['p', 'i']:

                        df_flux, en_channel_string =\
                            calc_av_en_flux_SEPT(self.current_df_i,
                                                 self.current_i_energies,
                                                 channels)
                    elif self.species == 'e':

                        df_flux, en_channel_string =\
                            calc_av_en_flux_SEPT(self.current_df_e,
                                                 self.current_e_energies,
                                                 channels)

            except KeyError:
                raise Exception(f"{channels} is an invalid channel or a combination of channels!")

        if self.spacecraft == 'soho':

            # A KeyError here is caused by invalid channel
            try:

                if self.sensor == 'erne':

                    if self.species in ['p', 'i']:

                        df_flux, en_channel_string =\
                            calc_av_en_flux_ERNE(self.current_df_i,
                                                 self.current_energies['channels_dict_df_p'],
                                                 channels,
                                                 species='p',
                                                 sensor='HET')

                if self.sensor == 'ephin':
                    # convert single-element "channels" list to integer
                    if type(channels) == list:
                        if len(channels) == 1:
                            channels = channels[0]
                        else:
                            print("No multi-channel support for SOHO/EPHIN included yet! Select only one single channel.")
                    if self.species == 'e':
                        df_flux = self.current_df_e[f'E{channels}']
                        en_channel_string = self.current_energies[f'E{channels}']

            except KeyError:
                raise Exception(f"{channels} is an invalid channel or a combination of channels!")

        if self.spacecraft == 'wind':
            if self.sensor == '3dp':
                # convert single-element "channels" list to integer
                if type(channels) == list:
                    if len(channels) == 1:
                        channels = channels[0]
                    else:
                        print("No multi-channel support for Wind/3DP included yet! Select only one single channel.")
                if self.species in ['p', 'i']:
                    if viewing != "omnidirectional":
                        df_flux = self.current_df_i.filter(like=f"FLUX_E{channels}_P{self.viewing[-1]}")
                    else:
                        df_flux = self.current_df_i.filter(like=f'FLUX_{channels}')
                    # extract pd.Series for further use:
                    df_flux = df_flux[df_flux.columns[0]]
                    # change flux units from '#/cm2-ster-eV-sec' to '#/cm2-ster-MeV-sec'
                    df_flux = df_flux*1e6
                    en_channel_string = self.current_i_energies['channels_dict_df']['Bins_Text'][f'ENERGY_{channels}']
                elif self.species == 'e':
                    if viewing != "omnidirectional":
                        df_flux = self.current_df_e[f"FLUX_E{channels}_P{self.viewing[-1]}"]
                        #df_flux = self.current_df_e.filter(like=f'FLUX_E{channels}')
                    else:
                        df_flux = self.current_df_e[f"FLUX_{channels}"]
                        #df_flux = self.current_df_e.filter(like=f'FLUX_{channels}')
                    # extract pd.Series for further use:
                    #df_flux = df_flux[df_flux.columns[0]]
                    # change flux units from '#/cm2-ster-eV-sec' to '#/cm2-ster-MeV-sec'
                    df_flux = df_flux*1e6
                    en_channel_string = self.current_e_energies['channels_dict_df']['Bins_Text'][f'ENERGY_{channels}']

        if self.spacecraft.lower() == "bepi":
            if type(channels) == list:
                if len(channels) == 1:
                    # convert single-element "channels" list to integer
                    channels = channels[0]
                    if self.species in ELECTRON_IDENTIFIERS:
                        df_flux = self.current_df_e[f"Side{viewing}_E{channels}"]
                        en_channel_string = self.current_energies[f"Side{viewing}_Electron_Bins_str"][f"E{channels}"]
                    if self.species in PROTON_IDENTIFIERS:
                        df_flux = self.current_df_i[f"Side{viewing}_P{channels}"]
                        en_channel_string = self.current_energies[f"Side{viewing}_Proton_Bins_str"][f"P{channels}"]
                    # Finally remove timezone-awareness from bepi data
                    df_flux.index = df_flux.index.tz_localize(None)
                else:
                    raise NotImplementedError("Combining channels not yet available for SIXS-P data!")
                    if self.species == 'e':
                        df_flux, en_channel_string = calc_av_en_flux_sixs(self.current_df_e, channels, self.species)
                    if self.species in ['p', 'i']:
                        df_flux, en_channel_string = calc_av_en_flux_sixs(self.current_df_i, channels, self.species)

        if self.spacecraft.lower() == 'psp':
            if self.sensor.lower() == 'isois-epihi':
                if self.species in ['p', 'i']:
                    # We're using here only the HET instrument of EPIHI (and not LET1 or LET2)
                    df_flux, en_channel_string =\
                        calc_av_en_flux_PSP_EPIHI(df=self.current_df_i,
                                                  energies=self.current_i_energies,
                                                  en_channel=channels,
                                                  species='p',
                                                  instrument='het',
                                                  viewing=viewing.upper())
                if self.species == 'e':
                    # We're using here only the HET instrument of EPIHI (and not LET1 or LET2)
                    df_flux, en_channel_string =\
                        calc_av_en_flux_PSP_EPIHI(df=self.current_df_e,
                                                  energies=self.current_e_energies,
                                                  en_channel=channels,
                                                  species='e',
                                                  instrument='het',
                                                  viewing=viewing.upper())
            if self.sensor.lower() == 'isois-epilo':
                if self.species == 'e':
                    # We're using here only the F channel of EPILO (and not E or G)
                    df_flux, en_channel_string =\
                        calc_av_en_flux_PSP_EPILO(df=self.current_df_e,
                                                  en_dict=self.current_e_energies,
                                                  en_channel=channels,
                                                  species='e',
                                                  mode='pe',
                                                  chan='F',
                                                  viewing=viewing)

        if self.spacecraft == 'solo':
            flux_series = df_flux #[channels]
        if self.spacecraft[:2].lower() == 'st':
            flux_series = df_flux  # [channel]'
        if self.spacecraft.lower() == 'soho':
            flux_series = df_flux  # [channel]
        if self.spacecraft.lower() == 'wind':
            flux_series = df_flux  # [channel]
        if self.spacecraft.lower() == 'psp':
            flux_series = df_flux #[channels]
        if self.spacecraft.lower() == 'bepi':
            flux_series = df_flux  # [channel]


        # Before returning, make sure that the type is pandas series, and not a 1-dimensional dataframe
        if not isinstance(flux_series, pd.core.series.Series):
            flux_series = flux_series.squeeze()

        return flux_series, en_channel_string


    def plot_subwindows(self):
        """
        Shows the subwindows used in the bootstrapping process
        """

        try:
            subwindows_plotted = self.subwindows_plotted
        except AttributeError:
            print("The onset object currently has no subwindows!")
            return 0

        fig, ax = self.fig, self.ax

        if not self.subwindows_plotted:
            for i, window in enumerate(self.list_of_bootstrap_windows):

                # Color the window
                ax.axvspan(xmin=window.start, xmax=window.end,
                            color=self.window_colors[i], alpha=0.25)

                # Draw the window boundaries with thin black lines
                ax.axvline(x=window.start, c='k', lw=1)
                ax.axvline(x=window.end, c='k', lw=1)

            # Keep track of wether subwindows have been plotted, to avoid over-plotting
            self.subwindows_plotted = True

        #plt.show()
        return fig


    def plot_cdf(self, prints=False) -> None:
        """
        Plots the Cumulatice Distribution Function (CDF) for a distribution of onset times.

        Parameters:
        -----------
        prints : bool, default False
                If True, will also print out the median onset time and the confidence intervals.
        """

        # Remember that onset times are sorted according to their probability in ascending order
        onset_timestamps = np.array([pair[0] for pair in self.bootstrap_onset_statistics["unique_onsets"]])
        probabilities = np.array([pair[1] for pair in self.bootstrap_onset_statistics["unique_onsets"]])

        # Here let's sort the onsets and probabilities in temporal order:
        onset_time_indices = onset_timestamps.argsort() # numpy array.argsort() returns a list of indices of values in ascending order

        onset_timestamps = pd.to_datetime(onset_timestamps[onset_time_indices])
        probabilities = probabilities[onset_time_indices]

        # Effectively calculate the cumulative sum of probabilities
        cumulative_probabilities = [np.nansum(probabilities[:i+1]) for i in range(len(probabilities))]
        
        # Get the median, mode, mean and confidence intervals (1sigma and 2sigma of normal distribution) of the onset distribution
        median_onset, _, _, confidence_intervals_all = self.get_distribution_statistics(onset_statistics=self.bootstrap_onset_statistics,
                                                                                    percentiles=[(15.89,84.1), (2.3,97.7)])

        # Flatten the sigma lists for plotting. Look at this list comprehension :D
        confidence_intervals_all = [timestamp for sublist in confidence_intervals_all for timestamp in sublist]

        # Init the figure
        fig, ax = plt.subplots(figsize=STANDARD_FIGSIZE)

        rcParams["font.size"] = 20

        # Settings for axes
        ax.xaxis_date()
        ax.set_xlabel("Time", fontsize=AXLABEL_FONTSIZE)
        ax.set_ylabel("Cumulative Probability", fontsize=AXLABEL_FONTSIZE)
        ax.set_title("Cumulative Distribution Function", fontsize=TITLE_FONTSIZE)
        hour_minute_format = DateFormatter("%H:%M")
        ax.xaxis.set_major_formatter(hour_minute_format)

        # Plot command(s)
        ax.step(onset_timestamps, cumulative_probabilities, zorder=3, c='k', where="post")

        colors = ("red", "red", "blue", "blue")
        # Horizontal dashed lines for 1-sigma and 2-sigma limits
        for i, num in enumerate((0.159, 0.841, 0.023, 0.977)):
            label = "15.9-84.1 percentiles" if num==0.841 else ("2.3-97.7 percentiles" if num==0.977 else None)
            ax.axhline(y=num, ls="--", c=colors[i], zorder=1, alpha=0.8, label=label)

        # Vertical dashed lines for 1-sigma and 2-sigma limits
        for i, time in enumerate(confidence_intervals_all):
            ax.axvline(x=time, ls="--", c=colors[i], zorder=1, alpha=0.8)

        ax.axvline(x=median_onset, ls="--", c="maroon", zorder=2, alpha=0.8, label="median onset (50th percentile)")
        ax.axhline(y=0.50, ls="--", c="maroon", zorder=1, alpha=0.8)
        
        ax.legend(loc=10, bbox_to_anchor=(1.27, 0.9))

        if prints:
            print(f"The median onset time is: {str(median_onset.time())[:8]}")
            # print(f"The np.percentile()-calculated median onset time (50th percentile) is: {str(median_onset.time())[:8]}")
            display(Markdown("~68 % confidence interval"))
            print(f"{str(confidence_intervals_all[0].time())[:8]} - {str(confidence_intervals_all[1].time())[:8]}")
            print(" ")
            display(Markdown("~95 % confidence interval"))
            print(f"{str(confidence_intervals_all[2].time())[:8]} - {str(confidence_intervals_all[3].time())[:8]}")

        plt.show()


    def scatter_histogram(self, xaxis:str="mean", xbinwidth:int|float=None, ybinwidth:str="1min",
                      hist_grids:bool=False, histy_logscale:bool=False,
                      return_fig_and_axes=False) -> None|tuple:
        """
        A method to plot a scatter and histogram plots of either background mean or background std
        vs. onset time. Always uses the statistics of the channel for which 'onset_statistics_per_channel()'
        was ran last.

        Parameters:
        -----------
        xaxis : {str}, optional
            Either 'mean' or 'std'. Default='mean'
        xbinwidth : {int,float}, optional
            The width of x-axis bins. Defaults to a fourth of the standard deviation of the sample.
        ybinwidth: {str}, optional
            Pandas-compatible time string for the width of onset distribution. Default='1min'
        hist_grids : {bool}, optional
            Switch for 'horizontal' gridlines on the histograms. Default=False
        histy_logscale : {bool}, optional
            Makes the onset histogram logscale. Default=False
        return_fig_and_axes : {bool}, optional
            Returns the figure and axes. Default=False
        """

        x_axes_choice = {
            "mean" : self.mu_and_sigma_distributions["mus_list"],
            "std" : self.mu_and_sigma_distributions["sigmas_list"]
        }

        xdata = x_axes_choice[xaxis]
        ydata = self.bootstrap_onset_statistics["onset_list"]

        title = r"Background $\mu$ vs onset time" if xaxis=="mean" else r"Background $\sigma$ vs onset time"
        xlabel = r"Sample mean intensity" if xaxis=="mean" else r"Sample standard deviation"
        ylabel = "Onset time [D HH:MM]"

        # definitions for the axes
        left, width = 0.1, 0.65
        bottom, height = 0.1, 0.65
        spacing = 0.005

        SUPTITLE_XOFFSET = 0.45
        SUPTITLE_YOFFSET = 1.0

        rect_scatter = [left, bottom, width, height]
        rect_histx = [left, bottom + height + spacing, width, 0.2]
        rect_histy = [left + width + spacing, bottom, 0.2, height]

        def scatter_hist(x, y, ax, ax_histx, ax_histy, histy_logscale:bool,
                         hist_grids:bool) -> None:

            # No labels for the histograms
            ax_histx.tick_params(axis="x", labelbottom=False)
            ax_histy.tick_params(axis="y", labelleft=False)

            # The scatter plot:
            for i, arr in enumerate(x):
                element = len(arr)
                ax.scatter(arr, y[i*element:element*(1+i)], color=self.window_colors[i], s=10)

            ylims = (np.min(y) - pd.Timedelta(minutes=2), np.max(y) + pd.Timedelta(minutes=2))
            ax.set_ylim(ylims)
            ax.grid(True)

            # Now determine the limits and binwidths
            if xbinwidth is None:
                # By default the binwidth is a fourth of the standard deviation of the sample
                binwidth = np.std(x)/4
            else:
                binwidth = xbinwidth

            xymax = np.nanmax(x) + 2*binwidth
            xmin = np.nanmin(x) - 2*binwidth

            xbins = np.arange(xmin, xymax, binwidth)
            for i, arr in enumerate(x):
                ax_histx.hist(arr, bins=xbins, color=self.window_colors[i], alpha=0.6)
            histx_yticks = ax_histx.get_yticks()
            ax_histx.set_yticks([tick for tick in histx_yticks if tick!=0])

            if hist_grids:
                ax_histx.grid(axis='y')

            half_bin = pd.Timedelta(seconds=30)
            ybins = pd.date_range(start=ylims[0]+half_bin, end=ylims[1]+half_bin, freq=ybinwidth).tolist()

            onset_frequencies = np.ones_like(y)/len(y)

            ax_histy.hist(y, bins=ybins, edgecolor="black", orientation='horizontal', weights=onset_frequencies)

            max_freq = np.nanmax([pair[1] for pair in self.bootstrap_onset_statistics["unique_onsets"]])

            if histy_logscale:
                ax_histy.set_xscale("log")
            else:
                ax_histy.set_xticks([np.round(max_freq*0.33,1), np.round(max_freq*0.66,1), np.round(max_freq,2)])

            histy_xticks = ax_histy.get_xticks()
            ax_histy.set_xticks([tick for tick in histy_xticks if tick<=1])

            if hist_grids:
                ax_histy.grid(axis='x')


        # Start with a square-shaped Figure
        fig = plt.figure(figsize=(12, 12))

        # These axes are for the scatter plot in the middle
        ax = fig.add_axes(rect_scatter)
        ax.yaxis_date()
        
        # These axes are for the histograms
        ax_histx = fig.add_axes(rect_histx, sharex=ax)
        ax_histy = fig.add_axes(rect_histy, sharey=ax)

        # Use the function defined above
        scatter_hist(xdata, ydata, ax, ax_histx, ax_histy, histy_logscale=histy_logscale,
                     hist_grids=hist_grids)
        
        ax.set_xlabel(xlabel, fontsize=AXLABEL_FONTSIZE)
        ax.set_ylabel(ylabel, fontsize=AXLABEL_FONTSIZE)

        fig.suptitle(t=f"{self.last_used_channel} {title}", x=SUPTITLE_XOFFSET, y=SUPTITLE_YOFFSET, fontsize=TITLE_FONTSIZE)

        plt.show()
        if return_fig_and_axes:
            return fig, ax


    def VDA(self, onset_times=None, Onset=None, energy:str='gmean', selection=None, 
            yerrs=None, reference:str="mode", title=None, ylim=None, plot=True, guess=None, save=False,
            savepath=None, fname:str=None, grid=True, show_omitted=True):
        """
        Performs Velocity Dispersion Analysis.

        Parameters:
        -----------
        onset_times : array-like, optional
                    List of onset times. If None, automatically acquired from self.onset_statistics
        Onset : self@Onset
                    The second onset object if joint-VDA
        energy: str, default 'gmean'
                    Mode of assessing nominal channel energy. 'gmean' or 'upper'
        selection: slice(start,stop) or a pair of slices, default: None
                    Selection of the channels one wishes to include in VDA calculation. If a pair of slices are delivered,
                    consider each slice to apply individually to each instrument respectively.
        yerrs : {array-like}, optional
                    Use provided y-errors instead of the ones in the object's memory.
        reference : {str}, either 'median' or 'mode' (default)
                    Which one to consider as the onset times extracted with the statistical method.
        title : {str}, default None
                    Custom title. If None, generate standard title.
        ylim : tuple/list with len==2, default None
                    the limits of y-axis.
        plot: {bool}, default True
                    Switch to produce plot of the analysis.
        guess : {array-like} of len()==2, default None
                    The initial guess values for the ODR fit. First one's the slope, which is a float, and the second is the
                    intersection point, which is a pandas-compatible datetime string.
        save: {bool}, default False
                    Switch to save the plotted figure. Only works if plot=True.
        fname : {str}, default None
                    A custom name for the figure if saved.
        grid : {bool} default True
                    Boolean switch for gridlines.
        show_omitted : {bool} default True
                    A switch to show the data points omitted from the fit in the plot.

        Returns:
        ---------
        output: dict
                    A dictionary containing  inverse_betas, x_errors, onset_times, y_errors, path_length, injection_time,
                    path_length uncertainty, injection_time uncertainty, residual variance (ODR),
                    stopreason (ODR), fig and axes.
        """

        VDA_MARKERSIZE = 24
        VDA_ELINEWIDTH = 2.2
        VDA_CAPSIZE = 6.0

        from scipy.stats import t as studentt

        import numpy.ma as ma

        spacecraft = self.spacecraft.lower()
        instrument = self.sensor.lower()
        species = self.species.lower()

        if species in ELECTRON_IDENTIFIERS:
            species_title = "electrons"
            m_species = const.m_e.value
        elif species in ("ion", 'i', 'h'):
            species_title = "ions"
            m_species = const.m_p.value
        elif species in ("protons", "proton", 'p'):
            species_title = "protons"
            m_species = const.m_p.value
        else:
            raise Exception(f"Particle species '{self.species}' does not appear to be any of the recognized species!")

        # E=mc^2, a fundamental property of an object with mass
        mass_energy = m_species*C_SQUARED # ~511 keV for electrons

        # Get the energies of each energy channel, to calculate the mean energy of particles and ultimately
        # to get the dimensionless speeds of the particles (v/c = beta).
        # These methods return the lower and higher energy bounds in *electron volts*
        if self.spacecraft in SEPPY_SPACECRAFT and not self.custom_data:
            e_min, e_max = self.get_channel_energy_values()
        else:
            e_min, e_max = self.get_custom_channel_energies()

        # Initialize the list of channels according to the sc and instrument used
        channels = self.get_all_channels()

        # SOHO /EPHIN really has only 4 active channels, but 5th one still exists, causing an erroneous amount
        # of channel nominal energies and hence wrong sized mask (5 instead of 4). For now we just discard
        # the final entries fo these lists.
        if spacecraft=="soho" and instrument=="ephin" and len(e_min)==5:
            e_min, e_max = e_min[:-1], e_max[:-1]

        # Get the nominal channel energies, which are by default the geometric means of channel boundaries.
        nominal_energies = calc_chnl_nominal_energy(e_min, e_max, mode=energy)

        # Check here if onset_times were given as an input. If not, use median/mode onset times and yerrs.
        onset_times = np.array([]) if onset_times is None else np.array(onset_times)
        if len(onset_times)==0:

            # Include a check here to get rid of channel 300 in EPHIN data after the switch-off
            if self.sensor == "ephin" and self.start_date > EPHIN_300_INVALID_ONWARDS:
                channels = [c_id for c_id in channels if c_id != 300]

            for ch in channels:
                # The median onset is found as the first entry of any list within the dictionary 'onset_statistics'. If that,
                # however, does not exist for a particular channel, then insert NaT to be masked away later in the method.
                try:
                    onset_times = np.append(onset_times,self.onset_statistics[ch][0])
                except KeyError as e:
                    onset_times = np.append(onset_times,pd.NaT)

        # Here check if two lists of onset times are being performed VDA upon
        if Onset:

            # The other set of onset times may belomng to a different particle species
            if Onset.species in ELECTRON_IDENTIFIERS:
                m_species1, species_title1 = const.m_e.value, "electrons"
            else:
                m_species1, species_title1 = const.m_p.value, "protons"
            mass_energy1 = m_species1 * C_SQUARED

            # First get the energies and nominal energies for the second instrument
            if Onset.spacecraft in SEPPY_SPACECRAFT:
                e_min1, e_max1 = Onset.get_channel_energy_values()
            else:
                e_min1, e_max1 = Onset.get_custom_channel_energies()

            # SOHO /EPHIN really has only 4 active channels, but 5th one still exists, causing an erroneous amount
            # of channel nominal energies and hence wrong sized mask (5 instead of 4). For now we just discard
            # the final entries fo these lists.
            if Onset.spacecraft=="soho" and Onset.sensor=="ephin" and len(e_min1)==5:
                e_min1, e_max1 = e_min1[:-1], e_max1[:-1]

            nominal_energies1 = calc_chnl_nominal_energy(e_min1, e_max1, mode=energy)

            # Check here if onset_times were given as an input. If not, use median/mode onset times and yerrs.
            if len(onset_times) == 2 and isinstance(onset_times[0], (list,tuple, np.ndarray)):

                onset_times1 = pd.to_datetime(onset_times[1])
                onset_times = pd.to_datetime(onset_times[0])

            else:

                # initialize the list of channels according to the sc and instrument used
                channels1 = Onset.get_all_channels()

                # Include a check here to get rid of channel 300 in EPHIN data after the switch-off
                if Onset.sensor == "ephin" and Onset.start_date > EPHIN_300_INVALID_ONWARDS:
                    channels1 = [c_id for c_id in channels1 if c_id != 300]

                # If there was no input for onset times, init an empty array and fill it up with values from the object
                onset_times1 = np.array([])

                for ch in channels1:

                    # The mode onset is found as the first entry of any list within the dictionary 'onset_statistics'. If that,
                    # however, does not exist for a particular channel, then insert NaT to be masked away later in the method.
                    try:
                        onset_times1 = np.append(onset_times1,Onset.onset_statistics[ch][0])
                    except KeyError as e:
                        onset_times1 = np.append(onset_times1,pd.NaT)


            onset_times_all = np.concatenate((onset_times, onset_times1))


            # Calculate the inverse betas corresponding to the nominal channel energies
            inverse_beta = calculate_inverse_betas(channel_energies=nominal_energies, mass_energy=mass_energy)
            inverse_beta1 = calculate_inverse_betas(channel_energies=nominal_energies1, mass_energy=mass_energy1)

            inverse_beta_all = np.concatenate((inverse_beta, inverse_beta1))

            # This is for the fitting function. 8.33 min = light travel time/au
            inverse_beta_corrected = np.array([8.33*60*b for b in inverse_beta_all]) #multiply by 60 -> minutes to seconds

            # Second values of the onset times
            date_in_sec = datetime_to_sec(onset_times=onset_times)
            date_in_sec1 = datetime_to_sec(onset_times=onset_times1)

            date_in_sec_all = np.concatenate((date_in_sec, date_in_sec1))

            # Error bars in x direction:
            x_errors_lower, x_errors_upper,  x_errors = get_x_errors(e_min=e_min, e_max=e_max, inverse_betas=inverse_beta, mass_energy=mass_energy)
            x_errors_lower1, x_errors_upper1,  x_errors1 = get_x_errors(e_min=e_min1, e_max=e_max1, inverse_betas=inverse_beta1, mass_energy=mass_energy1)

            # Arrays to hold all the lower and upper energy bounds. These might be redundant
            x_errors_lower_all = np.concatenate((x_errors_lower, x_errors_lower1))
            x_errors_upper_all = np.concatenate((x_errors_upper, x_errors_upper1))

            x_errors_all = np.concatenate((x_errors, x_errors1))

            # Loop through all possible, channels, even those that not necessarily show an onset
            # Remember:  self.onset_statistics : {channel_id : [mode, median, 1st_sigma_minus, 1st_sigma_plus, 2nd_sigma_minus, 2nd_sigma_plus]}
            if reference == "mode":
                ref_idx = 0
            elif reference == "median":
                ref_idx = 1
            else:
                raise ValueError(f"Argument {reference} is an invalid input for the variable 'reference'. Acceptable input values are are 'mode' and 'median'.")

            # Get all the y-errors there are in this object's database
            if not isinstance(yerrs, (list,np.ndarray)):

                plus_errs, minus_errs = np.array([]), np.array([])
                plus_errs1, minus_errs1 = np.array([]), np.array([])

                for ch in channels:

                    try: 
                        minus_err = self.onset_statistics[ch][ref_idx] - self.onset_statistics[ch][4] # the difference of a timestamp and a timedelta is a timedelta
                        plus_err = self.onset_statistics[ch][5] - self.onset_statistics[ch][ref_idx] 
                    except KeyError as e:
                        print(f"KeyError in channel {e}. Missing onset?")
                        # It's irrelevant what timedelta we insert here; the corresponding data point does not exist
                        plus_err = pd.Timedelta(seconds=1)
                        minus_err = pd.Timedelta(seconds=1)

                    # Before a check was done here to make sure that the errors are not smaller than half of the minimum
                    # cadence of the respective instrument; this is now obsolete, as the errors are properly inspected
                    # while the weighting is performed.
                    # plus_errs = np.append(plus_errs, plus_err) if plus_err >= self.minimum_cadences[f"{spacecraft}_{instrument}"]/2 else np.append(plus_errs, pd.Timedelta(self.minimum_cadences[f"{spacecraft}_{instrument}"])/2)
                    # minus_errs = np.append(minus_errs, minus_err) if minus_err >= self.minimum_cadences[f"{spacecraft}_{instrument}"]/2 else np.append(minus_errs, pd.Timedelta(self.minimum_cadences[f"{spacecraft}_{instrument}"])/2)
                    plus_errs = np.append(plus_errs, plus_err)
                    minus_errs = np.append(minus_errs, minus_err)


                for ch in channels1:

                    try: 
                        minus_err1 = Onset.onset_statistics[ch][ref_idx] - Onset.onset_statistics[ch][4] # the difference of a timestamp and a timedelta is a timedelta
                        plus_err1 = Onset.onset_statistics[ch][5] - Onset.onset_statistics[ch][ref_idx] 
                    except KeyError as e:
                        print(f"KeyError in channel {e}. Missing onset?")
                        # It's irrelevant what timedelta we insert here; the corresponding data point does not exist
                        plus_err1 = pd.Timedelta(seconds=1)
                        minus_err1 = pd.Timedelta(seconds=1)

                    plus_errs1 = np.append(plus_errs1, plus_err1)
                    minus_errs1 = np.append(minus_errs1, minus_err1)

                plus_errs_all = np.append(plus_errs, plus_errs1)
                minus_errs_all = np.append(minus_errs, minus_errs1)

                # Convert errors in time to seconds
                plus_errs_secs = [err.seconds for err in plus_errs]
                minus_errs_secs = [err.seconds for err in minus_errs]
                plus_errs_secs1 = [err.seconds for err in plus_errs1]
                minus_errs_secs1 = [err.seconds for err in minus_errs1]

                plus_errs_secs_all = np.append(plus_errs_secs, plus_errs_secs1)
                minus_errs_secs_all = np.append(minus_errs_secs, minus_errs_secs1)

                # Uneven errorbars need to be shape (2,N), where first row contains the lower errors, the second row contains the upper errors.
                y_errors_all = np.array([minus_errs_secs_all, plus_errs_secs_all])

            else:
                # The y-directional error was given by the user
                # 4 arrays, so asymmetric plus and minus errors
                if len(yerrs)==4:

                    plus_errs, minus_errs = np.array([]), np.array([])
                    plus_errs1, minus_errs1 = np.array([]), np.array([])

                    minus_timestamps, plus_timestamps = yerrs[0], yerrs[1]
                    minus_timestamps1, plus_timestamps1 = yerrs[0], yerrs[1]

                    for i, ch in enumerate(channels):

                        try: 
                            minus_err = minus_timestamps[i]
                            plus_err = plus_timestamps[i]
                        except KeyError as e:
                            plus_err = pd.Timedelta(seconds=1)
                            minus_err = pd.Timedelta(seconds=1)
                        
                        plus_errs = np.append(plus_errs, plus_err)
                        minus_errs = np.append(minus_errs, minus_err)

                    y_errors_all = np.array([plus_errs,minus_errs])

                    # Convert errors in time to seconds
                    plus_errs_secs = [err.seconds for err in plus_errs]
                    minus_errs_secs = [err.seconds for err in minus_errs]

                    y_errors_all_secs = np.array([plus_errs_secs,minus_errs_secs])

                # 2 arrays -> symmetric errors for both
                elif len(yerrs)==2:

                    # Check the first entry of yerrs. If it's not a timedelta, then it's the reach of the error
                    if not isinstance(yerrs[0][0], (pd.Timedelta, datetime.timedelta)):
                        y_errors_all = np.array([])  #= yerrs

                        for i, ch in enumerate(channels):
                            y_err = yerrs[i] - self.onset_statistics[ch][ref_idx]
                            y_errors_all = np.append(y_errors, y_err)

                    else:
                        # These are for the fitting function
                        y_errors = yerrs[0]
                        y_errors1 = yerrs[1]

                        y_errors_all = np.concatenate((y_errors, y_errors1))
                        y_errors_all_secs = [err.seconds for err in y_errors_all]

                # 1 array -> wrong
                else:
                    raise ValueError("The y-errors for two-instrument VDA must be in form of 4 or 2 lists!")

            # Numpy masks work so that True values get masked as invalid, while False remains unaltered
            mask = np.isnan(date_in_sec_all)

            # These can be used as maskedarray even when no invalid values are in the onset times
            date_in_sec_all = ma.array(date_in_sec_all, mask=mask)
            onset_times_all = ma.array(onset_times_all, mask=mask)
            inverse_beta_all = ma.array(inverse_beta_all, mask=mask)
            inverse_beta_corrected = ma.array(inverse_beta_corrected, mask=mask)
            x_errors_all = ma.array(x_errors_all, mask=mask)

            # Asymmetric errors / errors not defined by the user
            if len(y_errors_all)==2:
                y_errors_plot = ma.array([minus_errs, plus_errs], mask=[np.isnan(date_in_sec),np.isnan(date_in_sec)])
                y_errors1_plot = ma.array([minus_errs1, plus_errs1], mask=[np.isnan(date_in_sec1),np.isnan(date_in_sec1)])
                y_errors_all_plot = ma.array([minus_errs_all, plus_errs_all], mask=[mask,mask])
                y_errors_all_secs = ma.array([minus_errs_secs_all, plus_errs_secs_all], mask=[mask,mask])

            else:
                y_errors_plot = ma.array(y_errors, mask=np.isnan(date_in_sec))
                y_errors1_plot = ma.array(y_errors1, mask=np.isnan(date_in_sec1))
                y_errors_all_plot = ma.array(y_errors_all, mask=mask)
                y_errors_all_secs = ma.array(y_errors_all_secs, mask=mask)

            # After masking NaNs and Nats away, slice
            # which datapoints to consider for the fit
            if selection is not None and isinstance(selection[0],(slice,list)):

                if isinstance(selection[0],slice):
                    selection1 = selection[1]
                    selection = selection[0]

                    # The joint selection is a little tricky, as it requires concatenating two selections
                    bool_mask = [True if (i >= selection.start and i < selection.stop) else False for i in range(len(onset_times))]

                    # Check selection1 type to treat accordingly
                    if isinstance(selection1, slice):
                        bool_mask1 = [True if (i >= selection1.start and i < selection1.stop) else False for i in range(len(onset_times1))]
                    else:
                        bool_mask1 = selection1

                    # Finally combine selections
                    selection_all = np.append(bool_mask, bool_mask1)

                # In this case selection is a boolean [TRUE TRUE FALSE ...] list
                else:

                    selection1 = selection[1]
                    selection = selection[0]

                    # Check selection1 type to treat accordingly
                    if isinstance(selection1, slice):
                        selection1 = [True if (i >= selection1.start and i < selection1.stop) else False for i in range(len(onset_times1))]

                    # Finally combine selections
                    selection_all = np.append(selection,selection1) 

                # This variable signifies if there are data points in the plot that are not considered for the fit,
                # and are therefore hollowed with a white middle part.
                omitted_exists = True

            else:
                selection = slice(0,len(onset_times))
                selection1 = slice(0,len(onset_times1))
                selection_all = slice(0,len(onset_times_all))
                omitted_exists = False

            # These are only used for the fit -> slice them to fit selection
            inverse_beta_corrected = inverse_beta_corrected[selection_all]
            date_in_sec_all = date_in_sec_all[selection_all]

        # Only one instrument:
        else:

            # Calculate the inverse betas corresponding to the nominal channel energies
            inverse_beta = calculate_inverse_betas(channel_energies=nominal_energies, mass_energy=mass_energy)

            # This is for the fitting function. 8.33 min = light travel time/au , coeff = 8.33*60
            inverse_beta_corrected = np.array([b*8.33*60 for b in inverse_beta]) #multiply by 60 -> minutes to seconds

            # Get the second values for onset times for the fit
            date_in_sec = datetime_to_sec(onset_times=onset_times)

            # Error bars in x direction:
            x_errors_lower, x_errors_upper,  x_errors_all = get_x_errors(e_min=e_min, e_max=e_max, inverse_betas=inverse_beta, mass_energy=mass_energy)


            # Choose the reference index here before calculating the timedeltas for y-errors
            # self.onset_statistics : {channel_id : [mode, median, 1st_sigma_minus, 1st_sigma_plus, 2nd_sigma_minus, 2nd_sigma_plus]}
            if reference == "mode":
                ref_idx = 0
            elif reference == "median":
                ref_idx = 1
            else:
                raise ValueError(f"Argument {reference} is an invalid input for the variable 'reference'. Acceptable input values are are 'mode' and 'median'.")

            # Get all the y-errors there are in this object's database
            if not isinstance(yerrs, (list, np.ndarray)):

                plus_errs, minus_errs = np.array([]), np.array([])
                # Loop through all possible, channels, even those that not necessarily show an onset
                for ch in channels:

                    try: 
                        minus_err = self.onset_statistics[ch][ref_idx] - self.onset_statistics[ch][4] # the difference of a timestamp and a timedelta is a timedelta
                        plus_err = self.onset_statistics[ch][5] - self.onset_statistics[ch][ref_idx] 
                    except KeyError as e:
                        plus_err = pd.Timedelta(seconds=1)
                        minus_err = pd.Timedelta(seconds=1)

                    # The previous way of determining the validity of the ~95% confidence intervals here should be redundant now that the widths of 
                    # the intervals and their boundaries are already checked when they are determined. Instead, just collect the errors to the arrays.
                    # plus_errs = np.append(plus_errs, plus_err) if plus_err >= self.minimum_cadences[f"{spacecraft}_{instrument}"]/2 else np.append(plus_errs, pd.Timedelta(self.minimum_cadences[f"{spacecraft}_{instrument}"])/2)
                    # minus_errs = np.append(minus_errs, minus_err) if minus_err >= self.minimum_cadences[f"{spacecraft}_{instrument}"]/2 else np.append(minus_errs, pd.Timedelta(self.minimum_cadences[f"{spacecraft}_{instrument}"])/2)
                    plus_errs = np.append(plus_errs, plus_err)
                    minus_errs = np.append(minus_errs, minus_err)

                # Convert errors in time to seconds
                plus_errs_secs = [err.seconds for err in plus_errs]
                minus_errs_secs = [err.seconds for err in minus_errs]

                # Uneven errorbars need to be shape (2,N), where first row contains the lower errors, the second row contains the upper errors.
                y_errors_all = np.array([minus_errs, plus_errs])

            # User gave y-errors as an input
            else:

                # 2 arrays, so asymmetric plus and minus errors
                if len(yerrs)==2:

                    plus_errs, minus_errs = np.array([]), np.array([])

                    minus_timestamps, plus_timestamps = yerrs[0], yerrs[1]

                    for i, ch in enumerate(channels):

                        try: 
                            minus_err = minus_timestamps[i]
                            plus_err = plus_timestamps[i]
                        except KeyError as e:
                            plus_err = pd.Timedelta(seconds=1)
                            minus_err = pd.Timedelta(seconds=1)
                        
                        plus_errs = np.append(plus_errs, plus_err)
                        minus_errs = np.append(minus_errs, minus_err)

                    y_errors_all = np.array([plus_errs,minus_errs])

                    # Convert errors in time to seconds
                    plus_errs_secs = [err.seconds for err in plus_errs]
                    minus_errs_secs = [err.seconds for err in minus_errs]

                    y_errors_all_secs = np.array([plus_errs_secs,minus_errs_secs])

                # 1 array -> symmetric errors
                else:

                    # Check the first entry of yerrs. If it's not a timedelta, then it's the reach of the error
                    if not isinstance(yerrs[0], (pd.Timedelta, datetime.timedelta)):
                        y_errors_all = np.array([])  #= yerrs

                        for i, ch in channels:
                            y_err = yerrs[i] - self.onset_statistics[ch][ref_idx]
                            y_errors_all = np.append(y_errors, y_err)

                    else:
                        y_errors_all = yerrs

                    y_errors_all_secs = [err.seconds for err in y_errors_all]


            # Numpy masks work so that True values get masked as invalid, while False remains unaltered
            mask = np.isnan(date_in_sec)

            # These can be used as maskedarray even when no invalid values are in the onset times
            date_in_sec = ma.array(date_in_sec, mask=mask)
            onset_times = ma.array(onset_times, mask=mask)
            inverse_beta = ma.array(inverse_beta, mask=mask)
            inverse_beta_corrected = ma.array(inverse_beta_corrected, mask=mask)
            x_errors_all = ma.array(x_errors_all, mask=mask)

            # Errors are asymmetric
            if len(y_errors_all)==2:
                y_errors_plot = ma.array([minus_errs, plus_errs], mask=[mask,mask])
                y_errors_all_secs = ma.array([minus_errs_secs, plus_errs_secs], mask=[mask,mask])

            # Errors are symmetric
            else:
                y_errors_all_secs = ma.array(y_errors_all_secs, mask=mask)
                y_errors_plot = ma.array(y_errors_all, mask=mask)

            # Just to have a shared alias for all the y-errors, also in the case there are two instruments
            y_errors_all_plot = y_errors_plot

            # After masking NaNs and Nats away, slice
            # which datapoints to consider for the fit
            if selection is not None and isinstance(selection,(slice,list)):

                # User may give empty slice, e.g., slice(0,0) as an input. Handle it as if selection was None.
                if isinstance(selection,slice):
                    if selection.start==selection.stop:
                        selection_all = ~mask
                        selection = selection_all
                        omitted_exists = False
                
                selection_all = selection
                omitted_exists = True

            else:
                selection_all = ~mask
                selection = selection_all
                omitted_exists = False
                

            # These are only used for the fit -> slice them to fit selection
            inverse_beta_corrected = inverse_beta_corrected[selection_all]
            date_in_sec_all = date_in_sec[selection_all]

            # Common name to take into account single instrument and two-instrument code blocks
            inverse_beta_all = inverse_beta
            onset_times_all = onset_times

        # Here onward we do the fit, and find the slope and intersection of this fit ->

        # Here happens the fit and errors ->
        # x_errors and y_errors are fed in here with selections because we want to plot them for all data points, but only
        # consider the selection of them in the fit
        # The .compressed() -method here ensures that only the valid values of the masked arrays are fed into the function. If this is not
        # done, then all of the values that the function yields will be nan.
        odr_output = seek_fit_and_errors(x=inverse_beta_corrected.compressed(), y=date_in_sec_all.compressed(), 
                                        xerr=x_errors_all.compressed(), yerr=y_errors_all_secs.compressed(), guess=guess)
        
        # This would print out the output
        # odr_output.pprint()
        
        slope, constant = odr_output.beta[0], odr_output.beta[1]
        
        dof = len(inverse_beta_corrected) - len(odr_output.beta) # degrees of freedom (number of data points minus number of free parameters)
        t_val = studentt.interval(0.95, dof)[1] # 95 % confidence interval
        errors = t_val * odr_output.sd_beta
        slope_uncertainty = errors[0]

        try:
            t_inj_uncertainty = pd.Timedelta(seconds=int(errors[1]))

        # Valueerror is caused by nan
        except ValueError:
            t_inj_uncertainty = np.nan

        # Residual variance
        residual_variance = odr_output.res_var

        # Reason for stopping the regression:
        stopreason = odr_output.stopreason

        odr_fit_sec = inverse_beta_corrected*slope
        odr_fit = np.array([datetime.datetime.utcfromtimestamp(sec + constant) for sec in odr_fit_sec.compressed()])

        # Release time is the timestamp where the line intersects y-axis
        release_time = datetime.datetime.utcfromtimestamp(constant)

        # Precision of t_inj display:
        precision = 5 if spacecraft not in FINE_CADENCE_SC else 8
        rel_time_str =str(release_time.time())[:precision]

        date_of_event = get_figdate(onset_times_all.compressed())

        # Declare axes, the things plotted and returned by this function
        # First observed datapoints, then fit and its slope+constant, lastly y-and x errors

        output = {
                "inverse_beta" : inverse_beta_all,
                "x_errors" : x_errors_all,
                "onset_times" : onset_times_all,
                "y_errors" : y_errors_all_plot,
                "path_length" : slope,
                "t_inj" : release_time,
                "path_length_uncertainty" : slope_uncertainty,
                "t_inj_uncertainty" : t_inj_uncertainty,
                "residual_variance" : residual_variance,  # The unexplained variance of the data according to the model
                "stopreason" : stopreason
                }

        # Only plotting commands from here onward->
        if plot:

            plt.rcParams['axes.linewidth'] = 2.5
            plt.rcParams['font.size'] = 16

            # The colorblind style is apparently dependent on the configuration of stars and planets,
            # because sometimes 'seaborn-v0_8-colorblind works and sometimes it doesn't. So let's
            # try/except here to find a style that actually works.
            try:
                plt.style.use("seaborn-v0_8-colorblind")
            except OSError:
                plt.style.use("seaborn-colorblind")

            fig, ax = plt.subplots(figsize=VDA_FIGSIZE)

            ax.grid(visible=grid, axis="both")

            # Choose if to plot all data points, or just the ones that are considered for the fit. The default is to show all data points.
            if not show_omitted:
                omitted_exists = False
                inverse_beta = inverse_beta[selection]
                onset_times = onset_times[selection]
                x_errors_lower = x_errors_lower[selection]
                x_errors_upper = x_errors_upper[selection]
                
                # Compressing changes the shape of y_errors_plot. This has unintended consequences.
                # Lets instead try array.filled(np.nan), to fill in the invalid values with nan.
                # y_errors_plot = y_errors_plot.compressed()
                y_errors_plot = y_errors_plot.filled(np.nan)

                # Asymmetric y-errors
                if len(y_errors_plot)==2:

                    # y_errors_plot[0] = y_errors_plot[0][selection]
                    # y_errors_plot[1] = y_errors_plot[1][selection]
                    # Let's make an ugly implementation here via a temp array, because the old
                    # implementation (above) does not really work.
                    num_valids = len(inverse_beta)

                    tmp = np.empty(shape=(2,num_valids), dtype=pd.Timedelta)

                    tmp[0] = y_errors_plot[0][selection]
                    tmp[1] = y_errors_plot[1][selection]

                    # Lastly save tmp onto the old y_errors_plot while transforming possible
                    # nans to pd.NaTs, to make invalid values compatible with other datetime values.
                    # Logic: choose elements which are NOT notna (= not nan = timedeltas), replace
                    # those with pd.NaT and let the rest be as they are.
                    y_errors_plot = np.where(~pd.notna(tmp), pd.NaT, tmp)

                # Symmetric y-errors
                else:
                    y_errors_plot = y_errors_plot[selection]

                if Onset:
                    inverse_beta1 = inverse_beta1[selection1]
                    onset_times1 = onset_times1[selection1]
                    x_errors_lower1 = x_errors_lower1[selection1]
                    x_errors_upper1 = x_errors_upper1[selection1]
                    if len(y_errors1_plot)==2:
                        y_errors1_plot_tmp0 = y_errors1_plot[0][selection1]
                        y_errors1_plot_tmp1 = y_errors1_plot[1][selection1]
                        y_errors1_plot = [y_errors1_plot_tmp0, y_errors1_plot_tmp1]
                    else:
                        y_errors1_plot = y_errors1_plot[selection1]


            # About matplotlib.Axes.errorbar:
            # shape(2, N): Separate - and + values for each bar. First row contains the lower errors, the second row contains the upper errors.
            # The reason xerr seems to be wrong way is that 'upper' refers to the upper ENERGY boundary, which corresponds to the LOWER 1/beta boundary
            if Onset and len(inverse_beta1)>0:
                label1 = Onset.sensor.upper() if mass_energy==mass_energy1 else f"{Onset.sensor.upper()} {species_title1}"
                ax.errorbar(inverse_beta1, onset_times1, yerr=y_errors1_plot, xerr=[x_errors_upper1, x_errors_lower1], 
                        fmt='o', elinewidth=VDA_ELINEWIDTH, capsize=VDA_CAPSIZE, zorder=1, label=label1)

            if not Onset:
                label = "onset times"
            else:
                label = self.sensor.upper() if mass_energy==mass_energy1 else f"{self.sensor.upper()} {species_title}"

            if len(inverse_beta) > 0:
                ax.errorbar(inverse_beta, onset_times, yerr=y_errors_plot, xerr=[x_errors_upper, x_errors_lower], 
                            fmt='o', elinewidth=VDA_ELINEWIDTH, capsize=VDA_CAPSIZE, zorder=1, label=label)

            # Omitted datapoints, paint all points white and then those not omitted blue (+ red) again
            if omitted_exists:
                if Onset:
                    ax.scatter(inverse_beta1[selection1], onset_times1[selection1], s=VDA_MARKERSIZE, zorder=3)

                ax.scatter(inverse_beta_all, onset_times_all, c="white", s=VDA_MARKERSIZE-5, zorder=2)
                ax.scatter(inverse_beta[selection], onset_times[selection], s=VDA_MARKERSIZE, zorder=3)

            # The odr fit
            # Here we need to first take the selection of i_beta_all and ONLY after that take the compressed form, which is the set of valid values
            ax.plot(inverse_beta_all[selection_all].compressed(), odr_fit, ls='--',
                label=f"L: {np.round(slope,3):.3f} $\\pm$ {np.round(slope_uncertainty,3):.3f} AU{NEWLINE}t_inj: {rel_time_str} $\\pm$ {str(t_inj_uncertainty)[7:]}")

            ax.set_xlabel(r"1/$\beta$", fontsize = AXLABEL_FONTSIZE)

            # Format the y-axis. For that make a selection to exclude NaTs from the set of onset times that define 
            # the vertical axis boundaries.
            nat_onsets = pd.isnull(onset_times_all)
            not_nats = np.array(onset_times_all)[~nat_onsets]

            # We have to catch on the above line all non-NaT onset times, because numpy nanmin() and nanmax() don't recognize them
            if len(not_nats) > 0:
                if np.nanmax(not_nats)-np.nanmin(not_nats) > pd.Timedelta(minutes=10):
                    y_axis_time_format = DateFormatter("%H:%M")
                    ax.set_ylabel("Time (HH:MM)", fontsize = AXLABEL_FONTSIZE)
                else:
                    y_axis_time_format = DateFormatter("%H:%M:%S")
                    ax.set_ylabel("Time (HH:MM:SS)", fontsize = AXLABEL_FONTSIZE)
            ax.yaxis.set_major_formatter(y_axis_time_format)

            if ylim:
                ax.set_ylim(pd.to_datetime(ylim[0]),pd.to_datetime(ylim[1]))

            set_standard_ticks(ax=ax)
            set_legend(ax=ax, legend_loc="in", fontsize=LEGEND_SIZE)

            # Title for the figure
            if title is None:
                if Onset:
                    instrument_species_id = instrument.upper() if species_title==species_title1 else f"{instrument.upper()} {species_title}"
                    # This is the default for joint VDA, two instruments of the same spacecraft
                    if self.spacecraft == Onset.spacecraft:
                        if self.viewing:
                            ax.set_title(f"VDA, {spacecraft.upper()} / {instrument_species_id} ({self.viewing}) + {Onset.sensor.upper()} {species_title1}, {date_of_event}", fontsize=TITLE_FONTSIZE)
                        else:
                            ax.set_title(f"VDA, {spacecraft.upper()} / {instrument_species_id} + {Onset.sensor.upper()} {species_title1}, {date_of_event}", fontsize=TITLE_FONTSIZE)

                    else:
                        # In this case these are two different spacecraft
                        if self.viewing and Onset.viewing:
                            ax.set_title(f"VDA, {spacecraft.upper()}/{instrument_species_id} ({self.viewing}) + {Onset.spacecraft.upper()}/{Onset.sensor.upper()}({Onset.viewing}){NEWLINE}{species_title1}, {date_of_event}", fontsize=TITLE_FONTSIZE)
                        elif self.viewing:
                            ax.set_title(f"VDA, {spacecraft.upper()}/{self.sensor} ({self.viewing}) {species_title} + {Onset.spacecraft.upper()}/{Onset.sensor.upper()}{NEWLINE}{species_title1}, {date_of_event}", fontsize=TITLE_FONTSIZE)
                        elif Onset.viewing:
                            ax.set_title(f"VDA, {spacecraft.upper()}/{instrument_species_id} + {Onset.spacecraft.upper()}/{Onset.sensor.upper()}({Onset.viewing}){NEWLINE}{species_title1}, {date_of_event}", fontsize=TITLE_FONTSIZE)
                        else:
                            ax.set_title(f"VDA, {spacecraft.upper()}/{instrument_species_id} + {Onset.spacecraft.upper()}/{Onset.sensor.upper()} {species_title1}, {date_of_event}", fontsize=TITLE_FONTSIZE)

                else:
                    # Single spacecraft, single instrument
                    if self.viewing:
                        ax.set_title(f"VDA, {spacecraft.upper()} / {instrument.upper()} ({self.viewing}) {species_title}, {date_of_event}", fontsize=TITLE_FONTSIZE)
                    else:
                        ax.set_title(f"VDA, {spacecraft.upper()} / {instrument.upper()} {species_title}, {date_of_event}", fontsize=TITLE_FONTSIZE)
            else:
                ax.set_title(title, fontsize=TITLE_FONTSIZE)

            # Saving the figure in "/plots"
            if save:
                if not savepath:
                    savepath = CURRENT_PATH

                # If no custom name for the figure, generate it here
                if not isinstance(fname, str):
                    if Onset:
                        fname = f"{savepath}{os.sep}VDA_{spacecraft}_{instrument}({self.viewing})+{Onset.sensor}_{species_title}_{date_of_event}.png" if self.viewing else f"{savepath}{os.sep}VDA_{spacecraft}_{instrument}+{Onset.sensor}_{species_title}_{date_of_event}.png"
                    else:
                        fname = f"{savepath}{os.sep}VDA_{spacecraft}_{instrument}_{self.viewing}_{species_title}_{date_of_event}.png" if self.viewing else f"{savepath}{os.sep}VDA_{spacecraft}_{instrument}_{species_title}_{date_of_event}.png"

                save_figure(figure=fig, fname=fname, savepath=savepath)

            # Showing the figure (must be done AFTER saving)
            plt.show()

            # Add the figure and axes of it to the return
            output["fig"] = fig
            output["axes"] = ax

        return output


    def automatic_onset_stats(self, channels, background, viewing, erase, cusum_minutes:int=None, sample_size:float=0.5, 
                              small_windows=None, stop=None, weights="inverse_variance", 
                              limit_computation_time=True, sigma_multiplier=2, 
                              detrend:bool=True, prints:bool=False, custom_data_dt:str=None,
                              limit_averaging:str=None, fail_avg_stop:int=None, k_model=None):
        """
        Automates the uncertainty estimation for a single channel provided by Poisson-CUSUM-bootstrap hybrid method

        Parameters:
        ----------
        channels : {int}
                    Channel number.
        background : {BootstrapWindow}
                    The background that precedes the onset of the event.
        viewing : {str}
                    Viewing direction of the instrument.
        erase : {tuple(float, str)}
                    Acts a a filter that discards all values above the first entry BEFORE the timestamp defined by the second entry.
        cusum_minutes : {int}, optional
                    The amount of time in minutes that we demand on threshold-exceeding intensity.
        sample_size : {float} default 0.5
                    The fraction of the datapoints that the method will randomly pick.
        small_windows : {str} default None
                     Choose either 'random' or 'equal' to partition the big background window into smaller ones. 'equal' small windows
                     are equally placed inside the background, while 'random' placement allows the windows to overlap.
        stop : {str} default None
                    A pandas-compatible time string that decides on how long to continue averaging data while making onset 
                    distributions. If None, data will be averaged up to the 1-sigma uncertainty of whatever the native data resolution
                    produces.
        weights : {str} optional (default: 'inverse_variance')
                    Choose weights for calculating the mean uncertainty. 'inverse_variance' uses the inverse of the variance of the onset time distribution,
                    'int_time' will weight uncertainties by their individual integrating times, while 'uncertainty' will use the inverse of the ~95% of the
                    distribution as the weights.
        limit_computation_time : {bool}, default True
                    If enabled, skips all integration times above 10 minutes that are not multiples of 5. 
        sigma_multiplier : {int, float} default 2
                    The multiplier for the $\\mu_{d}$ variable in the CUSUM method.
        detrend : {bool}, default True
                    Switch to apply a shift on all but the native data distributions such that the onset times are shifted backwards in
                    time by half of the data time resolution.
        prints : {bool}, optional
                    Switch to print information about which channels is being analyzed and what's its 1-sigma uncertainty.
        custom_data_dt : {str}, optional
                    In case of custom data, provide the native cadence of the data PRIOR to resampling.
        limit_averaging : {str}, optional
                    Pandas-compatible time string. Limits the averaging to a certain time. Leave to None to not limit averaging. 
        fail_avg_stop : {int}, optional
                    If absolutely no onset is found in the native time resolution, how far should the method average the data to
                    try and find onset times? Default is up to 5 minutes.
        k_model : {Callable} Optional, default == None
                    Chooses the model for the k-parameter in Pyonset.
        Returns:
        ----------
        stats_arr : {OnsetStatsArray}
        """

        self.background = background

        def dt_str_to_int(stop):
            """
            Converts stop condition string to a integer.
            """

            if isinstance(stop, str):
                    if stop[-3:] == "min" or stop[-1:] == 'T':
                        split_str = "min"
                        divisor = 1
                    elif stop[-1:] == 'H':
                        split_str = "h"
                        divisor = 1/60
                    elif stop[-1:] == 's':
                        split_str = 's'
                        divisor = 60
                    else:
                        raise ValueError(f"{stop} is an incorrect value for parameter 'average_to'")
                    stop_int = int(stop.split(split_str)[0])//divisor
            else:
                raise ValueError("Time string has to pandas-compatible time string, e.g., '15 min' or '60 s'.")
            
            return stop_int


        def produce_integration_times(int_time_ints, limit_averaging, stop) -> list[str]:
            """
            Returns a list of integration time strings to be used in the loop that produces
            onset statistics arrays.
            """

            if limit_averaging is not None:
                unit_of_time = find_biggest_nonzero_unit(timedelta=pd.Timedelta(limit_averaging))
            elif stop is not None:
                unit_of_time = find_biggest_nonzero_unit(timedelta=pd.Timedelta(stop))
            else:
                raise Exception("""The unit of integration time not identified! The issue could be caused \
                                by custom data with unidentified frequency. Try either limiting the averaging with 'limit_averaging' \
                                keyword, or averaging to a set frequency with 'average_to' keyword.""")
            
            int_time_strs = [f"{i} {unit_of_time}" for i in int_time_ints]

            return int_time_strs


        # Check channel validity (if standard data)
        if not self.custom_data:
            if not isinstance(channels,int):
                if not isinstance(channels,list):
                    channels = int(channels)

        # Get the integer numbers that represent stop and/or limit_averaging in minutes
        if stop:
            # Integer value of stopping condition, e.g., the '5' in '5 min'.
            stop_int = dt_str_to_int(stop)
            # Unit of time
            unit_of_time = find_biggest_nonzero_unit(timedelta=pd.Timedelta(stop))
        if limit_averaging is not None:
            limit_averaging_int = dt_str_to_int(limit_averaging)
            # Unit of time
            unit_of_time = find_biggest_nonzero_unit(timedelta=pd.Timedelta(limit_averaging))

        # SolO/EPT first channel does not provide proper data as of late
        if self.spacecraft=="solo" and self.sensor=="ept" and channels==0:
            self.input_nat_onset_stats(channels)
            return  None

        # Wind/3DP first electron channel and the first two proton channels don't provide proper data
        if self.spacecraft=="wind" and self.species=='e' and channels==0:
            return  None
        if self.spacecraft=="wind" and self.species=='p' and channels in (0,1):
            return  None

        # The first round of onset statistics is acquired from 1 minute resolution, if computation time is limited, but 
        # only if data is NOT custom data.
        if self.spacecraft in FINE_CADENCE_SC and limit_computation_time and not self.custom_data:
            first_resample = "1 min"
        # Check if cadence is fine also for custom data
        elif self.custom_data:
            if custom_data_dt is None:
                custom_data_dt = self.data.index.freq if self.data.index.freq is not None else get_time_reso(self.data)

            # A fine cadence means less than 1 minute
            freq_is_fine = (pd.Timedelta(custom_data_dt) < pd.Timedelta("1 min"))

            first_resample = "1 min" if freq_is_fine and limit_computation_time else None

        else:
            first_resample = None

        # SOHO/EPHIN E300 is deactivated from 2017 onward -> there will be no reasonable onset there
        if self.spacecraft=="soho" and self.sensor=="ephin" and channels==300 and self.flux_series.index[0] >= EPHIN_300_INVALID_ONWARDS:
            print("Channel deactivated because of failure mode D.")
            return None

        # Run statistic_onset() once to get the confidence intervals for the bare not resampled, or 1-minute, data
        first_run_stats, _ = self.statistic_onset(channels=channels, Window=background, viewing=viewing, 
                                            sample_size=sample_size, resample=first_resample, erase=erase, small_windows=small_windows,
                                            cusum_minutes=cusum_minutes, detrend=False, sigma_multiplier=sigma_multiplier, k_model=k_model)

        # For the first iteration initialize the OnsetStatsArray object, which can plot the integration time plot
        # This step has to be done after running statistic_onset() the first time, because otherwise "self.bootstrap_onset_statistics"
        # does not exist yet
        stats_arr = OnsetStatsArray(self)

        # Integer number of first run uncertainty in minutes
        first_run_uncertainty_mins = (first_run_stats["1-sigma_confidence_interval"][1] - first_run_stats["1-sigma_confidence_interval"][0]).seconds // 60
        first_run_uncertainty = first_run_stats["1-sigma_confidence_interval"][1] - first_run_stats["1-sigma_confidence_interval"][0]

        if prints:
            print(f"~68 % uncertainty for the onset time with native data resolution: {first_run_uncertainty}")

        # Could be that the native resolution identifies no onset at all, in this case handle it
        if not isinstance(first_run_uncertainty, pd._libs.tslibs.nattype.NaTType):

            # If stop is not defined, then average up to predefined (default or first_run_uncertainty_mins) time
            if not stop:

                # Most of the high-energy particle instruments have a time resolution of 1 min, so don't do averaging for them
                # if uncertainty is something like 1 min 07 sec
                if  not first_run_uncertainty_mins > pd.Timedelta(self.native_resolution).seconds//60 and self.spacecraft not in FINE_CADENCE_SC:

                    stats_arr.calculate_weighted_uncertainty(weight_type="inverse_variance")

                    self.max_avg_times[channels] = pd.Timedelta(minutes=first_run_uncertainty_mins)

                    if prints:
                        print("No averaging")
                    return stats_arr

                else:
                    # SolO instruments and Wind/3DP have high cadence (< 1 min), so start integrating from 1 minute measurements
                    # unless limit_computation_time is enabled
                    if not self.custom_data:
                        if self.spacecraft in FINE_CADENCE_SC and not limit_computation_time:
                            start_idx = 1
                        else:
                            # If not, start resampling from native resolution + 1 minute
                            # For 1 min data this reads: 60//60 + 1 = 1 + 1 = 2
                            # For 2 min data this reads: 120//60 + 1 = 2 + 1 = 3
                            start_idx = pd.Timedelta(self.native_resolution).seconds//60 + 1

                    else:
                        # In case of custom data, start from the base cadence. Except for if the cadence is fine, then
                        # start from 1 minute
                        if freq_is_fine and not limit_computation_time:
                            start_idx = 1
                        
                        # If the cadence is not fine, then no matter if computation time is limited or not, we start
                        # time-averaging from the cadence_in_minutes+1 onward, e.g., 10 min -> 10+1 == 11 
                        else:
                            start_idx = pd.Timedelta(custom_data_dt).seconds//60 + 1

                    # Initialize integration times only up to the amount of minutes that the first run had uncertainty
                    int_times = np.array([i for i in range(start_idx,first_run_uncertainty_mins+1)], dtype=int)

                    if limit_averaging:
                            upto_averaging_display = limit_averaging_int if limit_averaging_int < first_run_uncertainty_mins else first_run_uncertainty_mins
                    else:
                        upto_averaging_display = first_run_uncertainty_mins

                    if prints:
                        print(f"Averaging from {start_idx} to {upto_averaging_display} minutes") if upto_averaging_display > 0 else print("Not averaging.")

            else:

                if stop_int > 0:
                    if prints:
                        print(f"Averaging up to {stop_int} minutes")

                # SolO instruments and Wind/3DP have high cadence (< 1 min), so start integrating from 1 minute measurements
                    # unless limit_computation_time is enabled
                if self.spacecraft in FINE_CADENCE_SC and not limit_computation_time:
                    int_times = np.array([i for i in range(1,stop_int+1)])
                else:
                    int_times = np.array([i for i in range(2,stop_int+1)])

        # Go here if no onset found at all
        else:

            if self.spacecraft in FINE_CADENCE_SC and not limit_computation_time:
                try_avg_start = 1
            else:
                try_avg_start = 2
            try_avg_stop = 5 if not isinstance(fail_avg_stop,int) else fail_avg_stop

            # Try up to {try_avg_stop} minutes averaging (default 5), if still no onset -> give up
            for i in range(try_avg_start,try_avg_stop+1):

                next_run_stats, _ = self.statistic_onset(channels=channels, Window=background, viewing=viewing, 
                                            sample_size=sample_size, resample=f"{i}min", erase=erase, small_windows=small_windows,
                                            cusum_minutes=cusum_minutes, sigma_multiplier=sigma_multiplier, detrend=True, k_model=k_model)
                next_run_uncertainty = next_run_stats["1-sigma_confidence_interval"][1] - next_run_stats["1-sigma_confidence_interval"][0]

                try:
                    next_run_uncertainty_mins = int(np.round((next_run_stats["1-sigma_confidence_interval"][1] - next_run_stats["1-sigma_confidence_interval"][0]).seconds / 60))
                except ValueError as e:
                    print(e)
                    print("This is caused by failing to identify the onset despite additional time-averaging.")
                    if i < try_avg_stop:
                        continue

                if not isinstance(next_run_uncertainty, pd._libs.tslibs.nattype.NaTType):
                    if prints:
                        print(f"No onset found in the native data resolution. ~68 % uncertainty with {i} min resolution: {next_run_uncertainty}")

                    # Here check if it makes sense to average "from i minutes to <uncertainty> minutes or up to "stop" minutes
                    if stop:

                        if i < stop_int:
                            int_times = np.array([j for j in range(i,stop_int+1)])
                            if prints:
                                print(f"Averaging from {i} minutes up to {stop_int} minutes")
                        else:
                            if prints:
                                print(f"Stop condition set to {stop_int} minutes, which is less than {i} min. Using only {i} minutes averaged data.")
                            int_times = np.array([j for j in range(i,i+1)])

                    elif i < next_run_uncertainty_mins:
                        int_times = np.array([j for j in range(i,next_run_uncertainty_mins+1)])
                        if prints:
                            if limit_averaging:
                                upto_averaging_display = limit_averaging_int if limit_averaging_int < next_run_uncertainty_mins else next_run_uncertainty_mins
                            else:
                                upto_averaging_display = next_run_uncertainty_mins

                            print(f"Averaging from {i} minutes up to {upto_averaging_display} minutes")

                    # No onset was found with any time averaging
                    else:
                        # if prints:
                          #   print(f"~68 % uncertainty less than current time-averaging. Terminating.")

                        self.max_avg_times[channels] = pd.NaT
                        stats_arr.add(self)
                        stats_arr.calculate_weighted_uncertainty(weights)
                        return stats_arr

                    break

                else:
                    # If we tried everything and still no onset -> NaT and exit
                    if i==try_avg_stop:
                        if prints:
                            print(f"No onsets found with 1 min ... {i} min time averaging. Terminating.")
                        self.max_avg_times[channels] = pd.NaT
                        stats_arr.calculate_weighted_uncertainty("int_time")
                        return stats_arr
                    else:
                        pass


        # Here if int_times gets too long, coarsen it up a little from 15 minutes onward
        # Logic of this selection: preserve an element of int_times if it's at most 15 OR if it's divisible by 5
        if limit_computation_time:
            int_times = int_times[np.where((int_times <= 15) | (int_times%5==0))]

        # If the user set some upper limit to the averaging, apply that limit here
        if isinstance(limit_averaging,str):
            int_times = int_times[np.where(int_times <= limit_averaging_int)]

        # Finally convert int_times (integers) to pandas-compatible time strs
        int_time_strs = produce_integration_times(int_time_ints=int_times, limit_averaging=limit_averaging, stop=stop)           

        # Loop through int_times as far as the first run uncertainty reaches
        for resample in int_time_strs:

            # A check to adjust the cusum window in cases of large integration times. Only for standardized data.
            if not self.custom_data:
                # Here it is assumed that the resample string represents minutes, and ends with 'min'.
                if int(resample[:-3]) > 10:
                    cusum_minutes = int(resample[:-3])*3

            _, _ = self.statistic_onset(channels=channels, Window=background, viewing=viewing, 
                                            sample_size=sample_size, resample=resample, erase=erase, small_windows=small_windows,
                                            cusum_minutes=cusum_minutes, sigma_multiplier=sigma_multiplier, detrend=detrend, k_model=k_model)

            stats_arr.add(self)

        # Add the final resampling time used to the dictionary of max_averaging times and 
        # the onset object otself to the stats_arr
        self.max_avg_times[channels] = pd.Timedelta(minutes=upto_averaging_display)

        # Calculate the weighted medians and confidence intervals. This method automatically updates the onset object's
        # dictionary of uncertainties as well.
        stats_arr.calculate_weighted_uncertainty(weights)

        return stats_arr


    def onset_statistics_per_channel(self, background, viewing=None, channels=None, erase:list=None, cusum_minutes:int=30, 
                                     sample_size:float=0.50, weights:str="inverse_variance", detrend=True, 
                                     limit_computation_time=True, average_to=None, print_output=False, 
                                     limit_averaging=None, fail_avg_stop:int=None, random_seed:int=None, 
                                     sigma_multiplier:int=2, k_model=None):
        """
        Wrapper method for automatic_onset_stats(), that completes full onset and uncertainty analysis for a single channel.
        Does a complete onset uncertainty analysis on, by default all, the energy channels for the given instrument.

        Parameters:
        -----------
        channels : {str or array-like}, optional
                    A tuple, list or Range of channel ID's to run the method on. Leave to None or set to 'all' to run for all channels.
        background : {BootstrapWindow}
                    The common pre-event background used for the energy channels. 
        viewing : {str}, optional
                    The viewing direction if the instrument. If not provided, use the default class attribute one.
        erase : {tuple(float, str)}, optional
                    If there are spikes in the background one wishes to omit, set the threshold value and ending point for ignoring those
                    values here. Example: [1000, '2022-05-10 12:00'] discards all values 1000 or above until 2022-05-10 noon.
        cusum_minutes : {int, float}, optional
                    The amount of MINUTES that the method will demand for continuous threshold-axceeding intensity before identifying
                    an onset. 
        sample_size : {float}, optional
                    The fraction of the data points inside the background window that will be considered for each of the bootstrapped
                    runs of the method.
        sigma : {int}, optional
                    The multiplier for the standard deviation, sigma, in the Poisson-CUSUM method. Defaults to 2.
        weights : {str}, optional (default: 'inverse_variance')
                    Pick 'inverse_variance' to use inverse variance weighting, 'uncertainty' to use the width of 2-sigma intervals as 
                    the basis for weighting timestamps, or 'int_time' to use the integration time as a basis for the weighting.
        detrend : {bool}, optional
                    Switch to apply detrending on the onset time distributions.
        limit_computation_time : {bool}, optional
                    If True (default), then resample all time series that have finer time resolution than 1 minute, to 1 minute resolution
                    before 
        average_to : {str}, optional
                    Explicitly tells the method to average every channel up to a specific time resolution, disregarding the
                    recommendation got from 1-sigma width of the native data. If both 'average_to' and 'limit_averaging' are
                    given as an input, 'limit_averaging' will take precedence over 'average_to'.
        print_output : {bool}
                    Switch to print when a new channel is being analyzed and for how far it will be time-averaged to.
        limit_averaging : {str}, optional
                    Pandas compatible time string to limit the averaging time to a certain time, e.g., '60 min'. if not provided,
                    then averaging will be limited by the recommended background.
        fail_avg_stop : {int}, optional
                    If absolutely no onset is found in the native time resolution, how far should the method average the data to
                    try and find onset times? Default is up to 5 minutes.
        random_seed : {int}, optional
                    Passes down a seed for the random generator that picks the samples from the background window.
        k_model : {Callable} Optional, default == None
                    Choose the model for the k-parameter in PyOnset.
        Returns:
        ----------
        uncertainty_stats_by_channel : {np.ndarray(OnsetStatsArray)}
                    A numpy array of OnsetStatsArray objects, each of which encapsulates statistics of the onset wime in each of the channels
                    that the method was run over. 
        """

        # Set the viewing even in the case it was not input by a user
        viewing = viewing if viewing is not None else self.viewing

        # Logic of this check:
        # If viewing was set (meaning viewing evaluates to True) and Onset.check_viewing() returns None 
        # (which evaluates to False), then viewing must be invalid.
        if viewing and not self.check_viewing(returns=True):
            raise ValueError("Invalid viewing direction!")


        # If a random seed (and a valid one!) was given, apply it before doing anything else
        if isinstance(random_seed,int):
            np.random.seed(random_seed)

        # Initialize the array which will store all the uncertaintystats objects
        uncertainty_stats_by_channel = np.array([])

        # Check which channels to run uncertainty stats on
        if channels is None or channels=="all":
            if self.custom_data:
                raise Exception("The list of channels must be manually given as an input in the case of custom data!")
            all_channels = self.get_all_channels()
        elif isinstance(channels, (tuple, list, range)):
            all_channels = channels
        elif isinstance(channels, (int, np.int64)):
            all_channels = [channels]
        elif isinstance(channels,str) and self.custom_data:
            all_channels = [channels]
        else:
            raise TypeError(f"{type(channels)} is and incorrect type of argument 'channels'! It should be None, str=='all', tuple, list or range.")

        # Recognize the base cadence here, before going in to automatic_onset_stats
        if self.custom_data:

            # Either the user hasn't provided a frequency to the data, or it's not possible due to irregular 
            # timedelta between data points. Try to infer the most prevalent timedelta, and assume that it is
            # a good representation of the time resolution.
            if self.data.index.freq is None:
                custom_data_dt = get_time_reso(self.data)

                if print_output:
                    print("Data frequency doesn't appear to be set. This can be done with set_data_frequency() -method.")
                    print(f"Inferring the most probable time resolution: {custom_data_dt}")
            else:
                custom_data_dt = self.data.index.freq
        else:
            custom_data_dt = None

        if print_output:
            background.print_max_recommended_reso()

        # If the time-averaging was not limited manually, limit it to the maximum recommended one
        if limit_averaging is None:
            limit_averaging = f"{background.max_recommended_reso} min"

        # Loop through all the channels and save the onset statistics to objects that will be stored in the array initialized in the start
        for channel in all_channels:

            if print_output:
                print(f"Channel {channel}:")

            # automatic_onset_stats() -method runs statistic_onset() for all different data integration times for a single channel
            onset_uncertainty_stats = self.automatic_onset_stats(channels=channel, background=background, viewing=viewing, erase=erase, sigma_multiplier=sigma_multiplier,
                                                                stop=average_to, cusum_minutes=cusum_minutes, sample_size=sample_size, 
                                                                weights=weights, detrend=detrend, limit_computation_time=limit_computation_time,
                                                                prints=print_output, limit_averaging=limit_averaging, fail_avg_stop=fail_avg_stop,
                                                                custom_data_dt=custom_data_dt, k_model=k_model)

            # Add statistics to the array that holds statistics related to each individual channel
            uncertainty_stats_by_channel = np.append(uncertainty_stats_by_channel, onset_uncertainty_stats)

        # Finally update the class attribute n
        self.sigma_multiplier = sigma_multiplier

        return uncertainty_stats_by_channel


    def find_first_peak(self, channels, window, xlim, viewing, resample, plot=True):
        """
        Finds the first maximum in time series data.
        
        Parameters:
        -----------
        window : {tuple(datetime str)}
        
        Returns:
        ----------
        first_maximum_peak_time : {pd.datetime}
        """

        flux_series, en_channel_string = self.choose_flux_series(channels=channels, viewing=viewing)

        # Resample data if requested
        if resample is not None:
            flux_series = util.resample_df(flux_series, resample)

        # Cut flux_series according to given window
        window = pd.to_datetime(window)
        cut_series = flux_series.loc[(flux_series.index > window[0]) & (flux_series.index < window[1])]

        # The series is indexed by time
        time = cut_series.index

        first_maximum_peak_val = cut_series.values[0]
        first_maximum_peak_time = time[0]
        for i, val in enumerate(cut_series.values):

            if val > first_maximum_peak_val:
                first_maximum_peak_val = val
                first_maximum_peak_time = time[i]

        # Plotting commands ->
        fig, ax = plt.subplots(figsize=(21,9))

        # Setting the x-axis limits
        if xlim is None:
            xlim = [flux_series.index[0], flux_series.index[-1]]
        else:
            # Check that xlim makes sense
            if xlim[0] == xlim[1]:
                raise Exception("xlim[0] and xlim[1] cannot be the same time!")

            xlim[0], xlim[1] = pd.to_datetime(xlim[0]), pd.to_datetime(xlim[1])
            ax.set_xlim(xlim)

        ax.set_yscale("log")

        # The measurements. kw where dictates where the step happens -> "mid" for at the middle of the bin
        ax.step(flux_series.index, flux_series.values, c="C0", where="mid")

        # Window shaded area
        ax.axvspan(xmin=window[0], xmax=window[1],
                    color="lightblue", label="Window", alpha=0.3)

        # Peak time, value and a textbox displaying both
        ax.axvline(x=first_maximum_peak_time, linewidth=1.2, color="navy", linestyle= '-', 
                    label="First maximum peak")

        # Textbox settings:
        plabel = AnchoredText(f"First maximum peak time+value{NEWLINE}{first_maximum_peak_time}{NEWLINE}{first_maximum_peak_val}", prop=dict(size=13), frameon=True, loc=(4) )
        plabel.patch.set_boxstyle("round, pad=0., rounding_size=0.2")
        plabel.patch.set_linewidth(2.0)
        ax.add_artist(plabel)

        # Labels for axes
        ax.set_xlabel("Time", fontsize=AXLABEL_FONTSIZE)
        ax.set_ylabel(f"Intensity [1/(cm^2 sr s MeV)]", fontsize=AXLABEL_FONTSIZE)

        # Tickmarks, their size etc...
        ax.tick_params(which='major', length=6, width=2, labelsize=17)
        ax.tick_params(which='minor', length=5, width=1.4)

        # Date tick locator and formatter
        ax.xaxis_date()
        utc_dt_format1 = DateFormatter('%H:%M {NEWLINE}%Y-%m-%d')
        ax.xaxis.set_major_formatter(utc_dt_format1)

        # Setting the title
        if self.species in ["electron", 'e']:
            s_identifier = 'electrons'
        if self.species in ["proton", 'p', 'H']:
            s_identifier = 'protons'
        if self.species in ["ion", 'i']:
            s_identifier = 'ions'

        if (viewing != '' and viewing is not None):

            plt.title(f"{self.spacecraft}/{self.sensor.upper()} {en_channel_string} {s_identifier}{NEWLINE}"
                    f"{resample} averaging, viewing: "
                    f"{viewing.upper()}")

        else:

            plt.title(f"{self.spacecraft}/{self.sensor.upper()} {en_channel_string} {s_identifier}{NEWLINE}"
                    f"{resample} averaging")

        if plot:
            plt.show()
        else:
            plt.close()

        return first_maximum_peak_time


    def get_distribution_statistics(self, onset_statistics:dict, percentiles:tuple = [(15.89,84.10), (2.30, 97.7)]):
        """
        Returns the median, mode, mean and confidence intervals of an onset distribution.
        
        Parameters:
        -----------
        onset_statistics : dict
                        A bootstrap_onset_statistics dictionary produced by the Onset class
        percentiles : tuple or a list of tuples
                        A tuple of percentiles, or a list containing the pairs of percentiles.
        
        Returns:
        -----------
        median : pandas datetime
                The median of the onset distribution
        mode : pandas datetime
                The mode of the onset distribution
        mean : pandas datetime
                The mean of the onset distribution
        confidence_intervals : tuple or a list of tuples
                Contains the percentiles or a list of percentiles if input was a list
        """

        onsets = pd.to_datetime(onset_statistics["onset_list"])

        # Calculate median with numpy.median
        median =  onsets[len(onsets)//2]

        # The most frequently appearing onset is already found
        mode = onset_statistics["unique_onsets"][-1,0]

        # Mean onset is already calculated
        mean = onset_statistics["mean_onset"]

        # Find the percentiles that were given
        confidence_intervals = self.get_distribution_percentiles(onsets, percentiles)

        return median, mode, mean, confidence_intervals


    def get_distribution_percentiles(self, onset_list, percentiles:tuple, time_reso=None):
        """
        Returns the confidence intervals of an onset distribution. Includes a check to make sure
        that the uncertainty intervals are not smaller than the data time resolution.
        
        Parameters:
        -----------
        percentiles : tuple or a list of tuples
                        A tuple of percentiles, or a list containing the pairs of percentiles.
        
        Returns:
        -----------
        confidence_intervals : tuple or a list of tuples
                Contains the percentiles or a list of percentiles if input was a list
        """

        # Find the percentiles that were given
        if isinstance(percentiles[0],(tuple,list)):
            confidence_intervals = []
            for pair in percentiles:
                conf_interval = pd.to_datetime(np.nanpercentile(onset_list,pair))
                confidence_intervals.append(conf_interval)
        else:
            confidence_intervals = pd.to_datetime(np.nanpercentile(onset_list,percentiles))

        # Finally check that the percentiles make sense (in that they are not less than the native data resolution of the instrument)
        if not time_reso:
            time_reso = self.get_minimum_cadence()
        confidence_intervals = check_confidence_intervals(confidence_intervals, time_reso=time_reso)

        return confidence_intervals


    def tsa_per_channel(self, radial_distance=None, path_length=None, solar_wind_speed=400, onset_times=None) -> dict:
        """
        Applies a time shift to all available energy channels accoridng to their kinetic energies
        and an assumed path of Parker spiral arc that they travelled.

        Parameters:
        -----------
        radial_distance : {float} The radial distance of the observer from the Sun at the time of observation.

        path_length : {float} Directly give the path length -> no calculation needed. This variable takes 
                              precedence over radial_distance.

        solar_wind_speed : {int/float} Solar wind speed in km/s. If not given, default to 400 km/s.

        onset_times : {dict} Give input onset times as a dictionary, coupling the channel identifier to the timestamp. 
                             If not given, use self.onset_statistics. (default=None)

        Returns: 
        ----------
        tsa_times : {dict} The time-shifted timestamps.
        """

        if radial_distance is None and path_length is None:
            raise TypeError("Either 'radial_distance' or 'path_length' must be specified to apply TSA!")

        # The nominal Parker spiral length for a given radial distance and solar wind speed
        if path_length is None:
            path_length = path_length_calculator(distance=radial_distance, solar_wind=solar_wind_speed)

        # The mean speeds of the energetic particles. The problem here is that the array
        # requires knowledge of the channel indices.
        if not self.custom_data:
            sep_speeds = self.calculate_particle_speeds()
        else:
            sep_speeds = self.calculate_particle_speeds_custom()

        if onset_times is None:
            onset_times = self.onset_statistics
        else:
            # Check that every timestamp is in a list
            try:
                _ = onset_times[list(onset_times.keys())[0]][0]
            except TypeError:
                onset_times = {channel:[timestamp] for (channel,timestamp) in onset_times.items()}

        # Init a dictionary to collect time-shifted timestamps
        tsa_times = {}

        # Enumerating a dictionary yields i, the index of a key, and the key itself.
        for i, channel in enumerate(onset_times):
            onset_stats = onset_times[channel]
            tsa_times[channel] = tsa(t0 = onset_stats[0], L=path_length, v=sep_speeds[i])

        return tsa_times


    def tsa_plot(self, radial_distance=None, path_length=None, solar_wind_speed=400, ylim=None, 
                 plot=True, save=False, savepath=None, onset_times=None):
        """
        Applies tsa on all channels, and produces a scatter plot.
        
        Parameters:
        -----------
        radial_distance : {float} Radial heliocentric distance in AUs. Either this or <path_length> has to be specified.

        path_length : {float} Distance for TSA in AUs. Either this or <radial_distance> has to be specified.
        
        solar_wind_speed : {int/float} Speed of the solar wind for Parker spiral caclulation in km/s.
                                        Defaults to 400 km/s if not given.
        
        ylim : {tuple/list} Set custom limits for y-axis.
        
        plot : {bool} Switch to produce a plot.
        
        save : {bool} Switch to save the plot.

        savepath : {str} A path to save the plot, if the parameter <save> is enabled.

        onset_times : {dict} User-input onset times. If not give, default to using self.onset_statistics
        
        Returns:
        --------
        tsa_results : {dict} 
        """

        if radial_distance is None and path_length is None:
            raise TypeError("Either 'radial_distance' or 'path_length' must be specified for TSA!")

        species_str = "electron" if self.species in ELECTRON_IDENTIFIERS else "proton"

        tsa_results = {}

        # Gets the time-shifted timestamps and save them to the dictionary
        tsa_timestamps = self.tsa_per_channel(radial_distance=radial_distance, path_length=path_length, 
                                              solar_wind_speed=solar_wind_speed, onset_times=onset_times)
        tsa_results["tsa_timestamps"] = tsa_timestamps

        # The x-axis in terms of the inverse speed:
        try:
            inverse_betas = np.array([const.c.value/v for v in self.calculate_particle_speeds()])

        # The UnboundLocalError arises due to seppy.calculate_particle_speeds() not recognizing the
        # energy values of the channels. Most likely because user has some custom data.
        except UnboundLocalError:
            inverse_betas = np.array([const.c.value/v for v in self.calculate_particle_speeds_custom()])

        # Stupid check and not general in its nature, but solo first channel is unavailable
        # so leave it out here
        if self.spacecraft.lower()=="solo" and self.sensor=="ept":
            inverse_betas = inverse_betas[1:]

        # Init the figure
        tsa_fig, tsa_ax = plt.subplots(figsize=STANDARD_FIGSIZE)

        # tsa_timestamps is a dictionary, so remember to only get the values for plotting
        tsa_ax.scatter(inverse_betas, tsa_timestamps.values(), s=135)

        tsa_ax.set_title(f"{self.spacecraft.upper()} / {self.sensor.upper()} {species_str} TSA", fontsize=TITLE_FONTSIZE)

        tsa_ax.yaxis.set_major_formatter(DateFormatter("%H:%M"))

        set_standard_ticks(ax=tsa_ax)
        
        tsa_ax.set_ylabel("Time", fontsize=AXLABEL_FONTSIZE)
        tsa_ax.set_xlabel(r"1/$\beta$", fontsize=AXLABEL_FONTSIZE)
        
        if ylim:
            tsa_ax.set_ylim(pd.to_datetime(ylim[0]), pd.to_datetime(ylim[1]))
        
        if save:
            if not savepath:
                savepath = CURRENT_PATH
            tsa_fig.savefig(f"{self.spacecraft.lower()}_{self.sensor.lower()}_{self.species}_tsa.png", facecolor="white", transparent=False,
                        bbox_inches="tight")
        if plot:
            plt.show()
        else:
            plt.close()

        tsa_results["inverse_betas"] = inverse_betas
        tsa_results["fig"] = tsa_fig
        tsa_results["axes"] = tsa_ax

        return tsa_results

    def calculate_particle_speeds_custom(self):
        """
        Calculates the average particle speeds by user-input channel energy boundaries.

        Note that the method set_custom_channel_energies() must have been ran before this method can be utilized.
        """

        if self.species in ELECTRON_IDENTIFIERS:
            m_species = const.m_e.value
        elif self.species in PROTON_IDENTIFIERS:
            m_species = const.m_p.value
        else:
            raise ValueError(f"The particle species {self.species} does not appear to be any of the recognized particle species. Can not calculate particle energy.")

        # E = mc^2, a fundamental property of any object with mass
        mass_energy = m_species * C_SQUARED
        
        # Get the energies of each energy channel, to calculate the mean energy of particles and ultimately
        # To get the dimensionless speeds of the particles (beta)
        e_lows, e_highs = self.get_custom_channel_energies()

        mean_energies = np.sqrt(np.multiply(e_lows, e_highs))

        # Transform kinetic energy from electron volts to joules
        e_Joule = [((En*u.eV).to(u.J)).value for En in mean_energies]

        # Beta, the unitless speed (v/c)
        beta = [np.sqrt(1-((e_J/mass_energy + 1)**(-2))) for e_J in e_Joule]

        return np.array(beta)*const.c.value

def get_x_errors(e_min, e_max, inverse_betas, mass_energy:float):
        """
        Calculates the width of energy channels in terms of the inverse speed (beta = v/c)

        Parameters:
        -----------
        e_min : array-like
                    The lower boundaries of energy channels in terms of eVs
        e_max : array-like
                    The higher boundaries of energy channels in terms of eVs
        inverse_betas : array-like
                    The inverse betas corresponding to channel nominal energies
        mass_energy : float
                    E = mc^2, the energy related to the mass of the particle

        Returns:
        -----------
        x_errors_lower : np.ndarray
                    The lower boundaries of energy channels in terms of inverse beta
        x_errors_upper : np.ndarray
                    The higher boundaries of energy channels in terms of inverse beta
        x_errors : np.ndarray
                    The widths of energy channels in terms of 1/beta
        """

        # Error bars in x direction:
        # First turn channel energy boundaries to Joules
        e_min_joule = [((e*u.eV).to(u.J)).value for e in e_min]
        e_max_joule = [((e*u.eV).to(u.J)).value for e in e_max]

        # Calculate beta from channel energy boundaries
        beta_lower = [np.sqrt(1-(( e_j/mass_energy + 1)**(-2))) for e_j in e_min_joule]
        beta_upper = [np.sqrt(1-(( e_j/mass_energy + 1)**(-2))) for e_j in e_max_joule]

        # The Inverse of beta
        beta_inv_lower = [1/x for x in beta_lower]
        beta_inv_upper = [1/x for x in beta_upper]

        # Calculate the difference between boundary and actual value
        x_errors_lower = [x-inverse_betas[indx] for indx, x in enumerate(beta_inv_lower)]
        x_errors_upper = [inverse_betas[indx]-x for indx, x in enumerate(beta_inv_upper)]

        x_errors = [x-x_errors_upper[i] for i, x in enumerate(x_errors_lower)]

        return np.array(x_errors_lower), np.array(x_errors_upper), np.array(x_errors)


def get_time_errors(onset_times, spacecraft):
    """
    Gets the errors in y-direction (time).

    Parameters:
    -----------
    onset_times : array-like
                All the onset times
    spacecraft : str
                The spacecraft identifier string.
    
    Returns:
    -----------
    time_error : list(pandas.Timedelta)
                A list of time errors as pandas-Timedelta objects
    """

    # Error in the y-direction (time):
    time_error = []

    for ts in onset_times:

        try:

            if isinstance(ts.freqstr,str):

                # freqstr could be negative, check!
                if ts.freqstr[0] == '-':
                    freqstr = ts.freqstr[1:]
                else:
                    freqstr = ts.freqstr
                time_error.append(pd.Timedelta(freqstr))

            else:
                if spacecraft in ["stereo-a", "stereo a", "stereo-b", "stereo b", "sta", "stb"]:
                    time_error.append(pd.Timedelta('1T'))
                else:
                    time_error.append(pd.Timedelta('1S'))

        except AttributeError:

            if spacecraft in ["stereo-a", "stereo a", "stereo-b", "stereo b", "sta", "stb"]:
                time_error.append(pd.Timedelta('1T'))
            else:
                time_error.append(pd.Timedelta('1S'))

    return time_error


def calculate_inverse_betas(channel_energies, mass_energy):
    """
    Calculates the inverse unitless speeds (1/beta = c/v) of particles given channel energies.

    Parameters:
    -----------
    channel_energies : array-like
                The nominal energies of energy channels.
    
    Returns:
    -----------
    inverse_betas : list
                The inverse betas corresponding to the nominal energies of the channels.
    """

    # Transform channel energies in eV to Joules
    e_Joule = [((En*u.eV).to(u.J)).value for En in channel_energies]

    # Beta, the unitless speed (v/c)
    beta = [np.sqrt(1-(( e_j/mass_energy + 1)**(-2))) for e_j in e_Joule]

    # Invert the beta values
    inverse_betas = np.array([1/b for b in beta])

    return inverse_betas


def tsa(t0:datetime.datetime, L:float, v:float):
    """
    Calculates the time shifted injection time assuming a path length L and particle speed v

    Parameters:
    -----------
    t0 : datetime.datetime
            The onset time of the event
    L : float
            The path length in AU
    v : float
            particle speed in m/s
    """

    path_in_meters = L * u.au.to(u.m)
    v_in_ms = v * u.m / u.s
    
    path_delta_sec = path_in_meters / v_in_ms

    t_inj = t0 - pd.Timedelta(seconds=path_delta_sec.value)

    return t_inj

def sample_mean_and_std(start, end, flux_series, sample_size=None, prints_warning=True):
    """
    Calculates the mean and standard deviation of the background period by
    taking a random sample of the background window of size sample_size * background_window.

    Parameters:
    -----------
    start: datetime
            Starting point of the calculation
    end: datetime
            Ending point of the calculation
    flux_series: pandas Series
            An array of observed values, indexed by datetime
    sample_size: float, default None
            A fraction of all the points to be considered in the calculation
            of mean and std. The sample is a random sample.
    prints_warning: bool, default True
            The function prints a warning if the sample size is small (data points < 100)

    Returns:
    --------
    (mean, std): tuple
            mean is the mean of the background period, std is its standard deviation
    """

    # Background contains all the points that could be considered for the calculation
    # of mean and std
    background = flux_series.loc[(flux_series.index >= start) & (flux_series.index < end)]

    # If sample_size is not provided, assume that all data points are picked
    if not sample_size:
        print("sample_size was not provided, assuming a 100 % sample.")
        sample_size = int(len(background))

    # Sample size is a fraction of the amount of background window data points
    else:
        if sample_size < 0.0 or sample_size > 1.0:
            raise Exception(f"The sample size must be between 0 and 1, not {sample_size}!")
        else:
            sample_size = int(sample_size * len(background))
            if len(background) < 100 and prints_warning:
                print(f"Warning, random sample is only {sample_size} data points!")


    # A random sample of all the background data points.
    sample = np.random.choice(background, replace=False, size=sample_size)

    # Calculate mean and std of the chosen sample of data points
    mean = np.nanmean(sample)
    std = np.nanstd(sample)

    return (mean, std)

def find_earliest_possible_onset(flux_series, cusum_function, onset_time):
    """
    Finds the earliest possible onset time by backtracking the cusum_function starting from the onset time and finding the
    first nonzero occurence of the cusum function.

    Parameters:
    -----------
    flux_series : pandas series
                    The series from which the onset was found. Intensity measurements indexed by datetimes.
    cusum_function : array-like
                    The values of the cusum function that is used to identify the onset time in the aforementioned time series.
    onset_time : datetime
                    The true onset time found from the time series with the cusum method.
    Returns:
    -----------
    earliest_possible_onset : datetime
                    The earliest possible onset time, identified by the first nonzero occurence of the cusum function 
                    backtracking from the true onset time.
    """
    # First to find the index of the onset, use numpy.where()
    datetimes = flux_series.index

    index_of_onset = np.where(datetimes == onset_time)[0][0]

    # Before looping througfh the cusum_function, flip it to make sure we are advancing in the past, not towards the future
    flipped_cusum_function = np.flip(cusum_function)

    # Init the earliest possible onset time
    earliest_possible_onset = onset_time

    # We'll go though the cusum_function, backtracking towards the past starting from the onset time
    for i in range(index_of_onset, len(flipped_cusum_function)):

        # The earliest possible onset time is the first nonzero occurence of cusum function PRIOR to the true onset
        if flipped_cusum_function[i] == 0:
            earliest_possible_onset = datetimes[i-1]
            break

    return earliest_possible_onset


# ===========================================================================================

def calc_chnl_nominal_energy(e_min, e_max, mode='gmean'):
    """
    Calculates the nominal energies of each channel, given the lower and upper bound of
    the channel energies. Channel energy should be given in eVs.

    Parameters:
    -----------
    e_min, e_max: np.arrays
            Contains lower and higher energy bounds for each channel respectively in eV
    mode: str, default='gmean'
        The mode for calculating nominal channel energy. Use 'gmean' for geometric mean, and
        'upper' for the upper energy bound.
    
    Returns:
    --------
    energies: np.array 
            Contains the nominal channel energy for each channel
    """

    # geometric mean
    if mode=='gmean':
        energies = np.sqrt(np.multiply(e_min,e_max))
        return energies
    
    # upper bound of energy channels
    elif mode=='upper':
        energies = e_max
        return energies
    
    else:
        raise Exception("Choose either 'gmean' or 'upper' as the nominal energy.")

#===========================================================================================

def path_length_calculator(distance, solar_wind = 400):
    """
    Calculates the length of the nominal parker spiral arm.

    Parameters:
    ------------
    distance : float, int
                heliospheric distance of the object in AU
    solar_wind = float, int
                solar wind speed in km/s, default = 400

    Returns:
    -----------
    path_length : float
                the path length in AU assuming the nominal Parker spiral
    """
    from sunpy.sun import constants as sunc

    # init the relevant constants
    OMEGA = sunc.sidereal_rotation_rate.value * (u.deg/u.day).to(u.rad/u.s) #rad/s, the equatorial sidereal rotation period of the Sun

    solar_wind_au = solar_wind * u.km.to(u.au)

    # These constants appears many times in the equation
    a = solar_wind_au/OMEGA
    psi = distance/a

    sqrt_term = np.sqrt(psi*psi + 1)

    # Calculate the length of the spiral arc
    path_length = 0.5 * a * ( psi*sqrt_term + np.log(psi + sqrt_term) ) # np.log() is the natural logarithm

    return path_length

#===========================================================================================

def flux2series(flux, dates, cadence=None):
    """
    Converts an array of observed particle flux + timestamps into a pandas series
    with the desired cadence.

    Parameters:
    -----------
    flux: an array of observed particle fluxes
    dates: an array of corresponding dates/times
    cadence: str - desired spacing between the series elements e.g. '1s' or '5min'

    Returns:
    ----------
    flux_series: Pandas Series object indexed by the resampled cadence
    """

    # from pandas.tseries.frequencies import to_offset

    # set up the series object
    flux_series = pd.Series(flux, index=dates)
    
    # if no cadence given, then just return the series with the original
    # time resolution
    if cadence is not None:
        try:
            flux_series = flux_series.resample(cadence, origin='start').mean()
            flux_series.index = flux_series.index + pd.tseries.frequencies.to_offset(pd.Timedelta(cadence)/2)
        except ValueError:
            raise Warning(f"Your 'resample' option of [{cadence}] doesn't seem to be a proper Pandas frequency!")

    return flux_series

# ======================================================================

def calculate_mean_and_std(start, end, flux_series):
    """
    This function calculates the mean and standard deviation of the background period
    which is used in the onset analysis. 

    Parameters:
    -----------
    start: datetime
            Starting point of the calculation
    end: datetime
            Ending point of the calculation
    flux_series: pandas Series
            An array of observed values, indexed by datetime

    Returns:
    --------
    (mean, std): tuple
            mean is the mean of the background period, std is its standard deviation
    """

    background = flux_series.loc[(flux_series.index >= start) & (flux_series.index < end)]

    mean = np.nanmean(background)
    std = np.nanstd(background)

    return (mean, std)

# =======================================================================

def erase_glitches(series, glitch_threshold, time_barr):
    """
    Every value above glitch_threshold before time_barr will be replace with nan.
    Also return the erased values in a series called "glitches", in case we need them for something, e.g. plotting.

    Parameters:
    -------------
    series : Pandas Series
                A series of intensity values indexed by pandas-compatible datetime-objects.
    glitch_threshold : float
                The maximum number above which in the right timeframe the values of the series are considered glitches.
    time_barr : datetime
                The time at which glitch_threshold stop applying.
    
    Returns:
    -------------
    series2 : Pandas Series
                The new series, with glitches omitted.
    glitches : Pandas Series
                All the found glitches collected to their own series, indexed by datetime.
    """

    if isinstance(time_barr,str):
        time_barr = pd.to_datetime(time_barr)

    # logically: if a value is BOTH before time_barr AND larger than glitch_threshold,
    # then the statement is false, and the corresponding value will be replaced with nan.
    glitches = series.loc[(series.values >= glitch_threshold) & (series.index < time_barr)]
    series2 = series.where((series.values < glitch_threshold) | (series.index >= time_barr))

    return series2, glitches

# ============================================================================

def onset_determination(ma_sigma, flux_series, cusum_window, avg_end, sigma_multiplier:int = 2,
                        k_model:str=None, norm:str='z') -> list :
    """
    Calculates the CUSUM function to find an onset time from the given time series data.

    Parameters:
    -----------
    ma_sigma : tuple(float, float)

    flux_series : pandas Series

    cusum_window : int

    avg_end : pandas datetime

    sigma_multiplier : float, int

    k_model : {Callable|str} Choose the model for the k-parameter. Callable k has to have identical signature
                to the k_parameter defined as a part of this software. String input 'legacy' uses the
                old and outdated definition of k, introduced in the papers Palmroos et al., 2022; Palmroos et al., 2025.

    norm : {str} How to normalize data. Either 'z' for z-standardizing, or 'sigma' for standardizing
                to the standard deviation.

    Returns:
    --------
    list(ma, md, k_round, h, norm_channel, cusum, onset_time)
    """

    # assert date and the starting index of the averaging process
    date = flux_series.index

    # First cut the part of the series that's before the ending of the averaging window. Then subtract that
    # from the size of the original series to get the numeral index that corresponds to the ending of the 
    # averaging window. starting_index is then the numeral index of the first datapoint that belongs outside the averaging window
    cut_series = flux_series.loc[flux_series.index > avg_end]
    start_index = flux_series.size - cut_series.size

    ma = ma_sigma[0]
    sigma = ma_sigma[1]
    md = ma + sigma_multiplier*sigma

    # The standard way of calculating k in SEPpy
    if k_model is None:
        k_round = k_parameter(mu=ma, sigma=sigma, sigma_multiplier=sigma_multiplier)
    # Non-default model for k
    elif k_model=="legacy":
        k_round = k_legacy(mu=ma, sigma=sigma, sigma_multiplier=sigma_multiplier)
    else:
        k_round = k_model(mu=ma, sigma=sigma, sigma_multiplier=sigma_multiplier)

    # choose h, the variable dictating the "hastiness" of onset alert
    h = 2 if k_round>1 else 1

    # Tracks the number of consecutive alert signals given
    alert = 0

    # A counter for the amount of nans subsequent to the last time the warning signal
    # turned from 0 to 1
    current_nan_streak = 0

    cusum = np.zeros(len(flux_series))

    if norm=='z':
        norm_channel = z_score(series=flux_series, mu=ma, sigma=sigma)
    elif norm=="sigma":
        norm_channel = sigma_norm(series=flux_series, sigma=sigma)
    else:
        raise ValueError("Parameter norm has to be either 'z' or 'sigma'!")

    # set the onset as default to be NaT (Not a daTe)
    onset_time = pd.NaT

    # start at the index where averaging window ends
    for i in range(start_index,len(cusum)):

        # If there are gaps in the data (nans) we want to ignore them.
        # This is done by keeping cusum constant over data gaps. 
        # During gaps alert signals are not modified, i.e., not incremented nor set to zero. Also the
        # cusum function stays constant.
        # In addition, the current nan streak IS incremented.
        if np.isnan(norm_channel[i]):
            cusum[i] = cusum[i-1]
            current_nan_streak += 1
            continue

        # Calculate the value for the next cusum entry
        cusum[i] = max(0, norm_channel[i] - k_round + cusum[i-1])

        # Check if cusum[i] is above threshold h, if it is -> increment alert
        if cusum[i]>h:
            alert += 1
        # If not, restart the counting of alert signals and the nan counter.
        else:
            alert=0
            current_nan_streak = 0

        # When the number of alerts exactly matches cusum_window, we have found the onset. Now to choose the correct
        # timestamp we need to backtrack from the current timestamp first the number of alerts, and then also
        # the number of nans subsequent to the first alert.
        if alert == cusum_window:
            onset_time = date[i - alert - current_nan_streak]
            break

    # ma = mu_a = background average
    # md = mu_d = background average + 2*sigma
    # k_round = integer value of k, that is the reference value to poisson cumulative sum
    # h = 1 or 2,describes the hastiness of onset alert
    # onset_time = the time of the onset

    return [ma, md, k_round, h, norm_channel, cusum, onset_time]


def onset_determination_cr(ma_sigma, flux_series, cusum_window, avg_end, sigma_multiplier : int = 2) -> list :
    """
    Calculates the CUSUM function to find an onset time from a given count rate data.
    
    The essential difference to the 'onset_determination()' -function is that this does NOT
    employ z-standardized intensity. rather it simply uses the counts for calculating
    CUSUM. Also k is not normalized to standard deviation. 

    Parameters:
    -----------
    ma_sigma : tuple(float, float)

    flux_series : pandas Series

    cusum_window : int

    avg_end : pandas datetime

    sigma_multiplier : float, int

    Returns:
    --------
    list(ma, md, k_round, h, flux_series.values, cusum, onset_time)
    """

    # Assert date and the starting index of the averaging process
    date = flux_series.index

    # First cut the part of the series that's before the ending of the averaging window. Then subtract that
    # from the size of the original series to get the numeral index that corresponds to the ending of the 
    # averaging window. starting_index is then the numeral index of the first datapoint that belongs outside
    # the averaging window
    cut_series = flux_series.loc[flux_series.index > avg_end]
    start_index = flux_series.size - cut_series.size

    ma = ma_sigma[0]
    sigma = ma_sigma[1]

    md = ma + sigma_multiplier*sigma

    # k may get really big if sigma is large in comparison to mean
    # try:
    #     k = (md-ma)/(np.log(md)-np.log(ma))

    #     # If ma == 0, then std == 0. Hence CUSUM should not be restricted at all -> k_round = 0
    #     # Otherwise k_round should be 1
    #     if not np.isnan(k):
    #         k_round = round(k) if k > 1 else k
    #     else:
    #         k_round = 1 if ma > 0 else 0

    # except (ValueError, OverflowError) as error:
    #     # the first ValueError I encountered was due to ma=md=2.0 -> k = "0/0"
    #     # OverflowError is due to k = inf
    #     # print(error)
    #     k_round = 1 if ma > 0 else 0
    k_round = k_classic(mu=ma, sigma=sigma, sigma_multiplier=sigma_multiplier)

    # Choose h, the variable dictating the "hastiness" of onset alert
    h = 2 if k_round>1 else 1

    alert = 0
    cusum = np.zeros(len(flux_series))
    
    # Set the onset as default to be NaT (Not a daTe)
    onset_time = pd.NaT

    # Start at the index where averaging window ends
    for i in range(start_index,len(cusum)):

        # Calculate the value for ith cusum entry
        cusum[i] = max(0, flux_series[i] - k_round + cusum[i-1])

        # check if cusum[i] is above threshold h, if it is -> increment alert
        if cusum[i]>h:
            alert += 1
        else:
            alert = 0

        # cusum_window(default:30) subsequent increments to alert means that the onset was found
        if alert == cusum_window:
            onset_time = date[i - alert]
            break

    # ma = mu_a = background average
    # md = mu_d = background average + 2*sigma
    # k_round = integer value of k, that is the reference value to poisson cumulative sum
    # h = 1 or 2,describes the hastiness of onset alert
    # onset_time = the time of the onset

    return [ma, md, k_round, h, flux_series.values, cusum, onset_time]


#==============================================================================================

def bootstrap_mus_and_sigmas(flux_series, window_start, window_end, n_bstraps, sample_size=None):
    """
    The function looks at flux_series through a time window, and extracts n_bstraps samples
    from it. The size of a sample size <sample_size> must not be greater than the amount of data points
    inside the time window. It then calculates the mean and standard deviation of those samples and returns
    an array of means and an array of stds.
    """ 
    
    # make a selection including only the measurements in the window
    window = flux_series.loc[(flux_series.index>=window_start) & (flux_series.index<window_end)]

    if sample_size is None:
        #default sample size is set to 50 % of total datapoints in a window
        sample_size = int(len(window)/2)
    
        # Print out a warning if the amount of datapoints in a window is less than 100
        # This is done ONLY IF no sample size is provided
        if len(window)<100:
            print(f"Warning! The ensemble of background values is {len(window)}<100. {NEWLINE}Raising sample size to 75 %.")
            sample_size = int(3*len(window)/4)

    # if sample_size is not None, check that it's a reasonable value
    else:
        if sample_size < 0.0 or sample_size > 1.0:
            raise Exception(f"The sample size must be between 0 and 1, not {sample_size}!")
        else:
            sample_size = int(sample_size * len(window))
            if len(window) < 100:
                print(f"Warning, random sample is only {sample_size} data points!")

    # initialize arrays
    mean_arr = np.zeros(n_bstraps)
    std_arr = np.zeros(n_bstraps)

    # repeat n_bstraps times
    for i in range(n_bstraps):

        # look inside the window and randomly pick sample_size amount of measurements there
        sample = np.random.choice(window, replace=True, size=sample_size)
        
        # then calculate the mean and standard deviation of that sample
        mean_arr[i] = np.nanmean(sample)
        std_arr[i] = np.nanstd(sample)


    return mean_arr, std_arr

#================================================================================

def unique_vals(onset_list, prints=False):
    '''
    Returns all the unique values from a list of onset times, and the corresponding counts. Counts also include
    the amount of NaT values at the end of the array, which is why len(uniques) == len(counts)-1.

    Parameters:
    -----------
    onset_list : list
                A list of onset times
    prints : bool, default False
                Switch to print out the relative abundances of each onset in the list
    
    Returns:
    -----------
    uniques : np.array
                An array of the UNIQUE appearances of the onset times in the input list
    counts : np.array
                An array of the number of appearances of each onset in the input list + the amount of NaTs.
    '''
    
    # Convert to pandas datetimes to ensure functionality with NaTs
    onsets = pd.to_datetime(onset_list)
    
    # Construct a mask, which is True on NaT values
    natmask = pd.isnull(onsets)
    
    # Also keep track of the amount of NaTs
    nat_amount = len(np.where(natmask)[0])
    
    # Use the NOT mask to get all the values which are NOT NaTs
    masked_onsets = onsets[~natmask]
    
    # Sort the times in chronological order
    masked_onsets = np.sort(masked_onsets)
    
    # Returns unique values from the masked array, and an array corresponding to the 
    # mount of each unique instance
    uniques, counts = np.unique(masked_onsets, return_counts=True)
    
    # ...Again convert to pandas datetimes
    uniques = pd.to_datetime(uniques)
    
    # Add the amount of NaTs to the end of counts
    counts = np.append(counts, nat_amount)

    count_sum = sum(counts)
    
    if prints:
        for i, uni_val in enumerate(uniques):
            print(f"{str(uni_val)[:-4]},  n={counts[i]}  ({np.round(100*counts[i]/count_sum, 2)} %)")

        print(f"NaTs: {counts[-1]}  ({np.round(100*counts[-1]/count_sum, 2)} %)")
        print("#----------------------------------")
    
    # Check that not ALL onsets are NaTs (this is evident if counts only holds one number)
    if len(counts) == 1:
        print(f" function unique_vals() output: {onsets[0]}, {counts}")
        return [onsets[0]], counts
    else:
        return uniques, counts

#================================================================================================================

def sc_distance(mag_data):

    from sunpy.coordinates import frames, get_horizons_coord

    pos = get_horizons_coord("Solar Orbiter", mag_data.index, "id")  # (lon, lat, radius) in (deg, deg, AU)
    pos = pos.transform_to(frames.HeliographicCarrington)
    dist = np.round(pos.radius.value, 2)

    return dist

#==============================================================================

def seek_fit_and_errors(x,y,xerr,yerr, guess=None):
    '''
    Seeks a fit and its errors by ODR (Orthogonal Distance Regression) 
    https://www.tutorialspoint.com/scipy/scipy_odr.htm

    Parameters
    -----------
    x, y :  arrays
            values for the x and y axes
    xerr, yerr : array
            errors of x and y, respectively
    guess : {array-like, list, tuple} with len()==2, default None
            The first guess values for the ODR fit.

    Returns:
    ----------
    out : {ODR.regression}
            An object containing information of the fit and it's errors
    '''

    # Define a polynomial of rank 1 to fit the data with.
    def linear_func(p, x):
        slope, const = p
        return slope*x + const

    # Create a model for fitting.
    linear_model = Model(linear_func)

    # Create a RealData object using our initiated data from above.
    data = RealData(x, y, xerr, yerr)

    if not guess:
        slope0, const0 = 1.0, y[-1]
    else:
        slope0, const0 = guess[0], datetime_to_sec([pd.to_datetime(guess[1])])[0]

    # Set up ODR with the model and data.
    odr = ODR(data, linear_model, beta0=[slope0, const0])

    # Run the regression.
    out = odr.run()

    # The built-in pprint method can be used to show the results
    # out.pprint()

    return out


def event_params_to_csv(event_params:dict, filename:str) -> None:
    """
    Saves the event parameters of the hybrid method to a csv file.

    Parameters:
    -----------
    event_params : {dict}
                    A dictionary that contains the event parameters. The dictionary is 
                    compiled within the .final_onset_plot() -method.
    filename : {str}
                    The name of the csv file.
    """

    TIME_INTERVAL_CONNECT = " -- "
    DT_SAVE_FORMAT = "%Y-%m-%d %H:%M:%S" 

    columns = []
    for name in event_params.keys():

        if name == "confidence_interval1":
            name = "~68% error"
        if name == "confidence_interval2":
            name = "~95% error"
        if name == "peak_intensity":
            name = f"{name} [1/cm^2 sr s MeV]"

        columns.append(name)

    new_values = []
    for value in event_params.values():

        # In case of the confidence intervals; they come in pairs stored in lists
        if isinstance(value, list):
            new_value = f"{value[0].strftime(DT_SAVE_FORMAT)}{TIME_INTERVAL_CONNECT}{value[1].strftime(DT_SAVE_FORMAT)}"
            new_values.append(new_value)
            continue

        new_values.append(value)

    df = pd.DataFrame(data=[new_values], columns=columns)

    df.to_csv(filename, index=False)
    del df


def _isnotebook():
    # https://stackoverflow.com/a/39662359/2336056
    try:
        shell = get_ipython().__class__.__name__
        if shell == 'ZMQInteractiveShell':
            return True   # Jupyter notebook or qtconsole
        elif shell == 'TerminalInteractiveShell':
            return False  # Terminal running IPython
        else:
            return False  # Other type (?)
    except NameError:
        return False

# ============================================================================================================================
# this will be ran when importing
# set Jupyter notebook cells to 100% screen size:
if _isnotebook():
    python_major_vers, python_minor_vers = sys.version_info[0:2]
    if python_major_vers < 3:
        print(f"Detecting user Python version: {sys.version}")
        raise ImportError("Python major version < 3 is not supported!")
    if python_minor_vers < 9:
        print(f"Detecting user Python version: {sys.version}")
        raise ImportError("Python versions < 3.9 are not compatible with IPython > 7.14. Importing display impossible.")
    else:
        from IPython.display import HTML, display
        display(HTML(data="""<style> div#notebook-container { width: 99%; } div#menubar-container { width: 85%; } div#maintoolbar-container { width: 99%; } </style>"""))
